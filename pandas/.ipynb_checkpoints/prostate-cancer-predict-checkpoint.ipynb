{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('F:/ml-data/prostate-cancer/datasets_66762_131607_Prostate_Cancer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis_result</th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>compactness</th>\n",
       "      <th>symmetry</th>\n",
       "      <th>fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>151</td>\n",
       "      <td>954</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>133</td>\n",
       "      <td>1326</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>M</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>130</td>\n",
       "      <td>1203</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>78</td>\n",
       "      <td>386</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>M</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>135</td>\n",
       "      <td>1297</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id diagnosis_result  radius  texture  perimeter  area  smoothness  \\\n",
       "0   1                M      23       12        151   954       0.143   \n",
       "1   2                B       9       13        133  1326       0.143   \n",
       "2   3                M      21       27        130  1203       0.125   \n",
       "3   4                M      14       16         78   386       0.070   \n",
       "4   5                M       9       19        135  1297       0.141   \n",
       "\n",
       "   compactness  symmetry  fractal_dimension  \n",
       "0        0.278     0.242              0.079  \n",
       "1        0.079     0.181              0.057  \n",
       "2        0.160     0.207              0.060  \n",
       "3        0.284     0.260              0.097  \n",
       "4        0.133     0.181              0.059  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>compactness</th>\n",
       "      <th>symmetry</th>\n",
       "      <th>fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>50.500000</td>\n",
       "      <td>16.850000</td>\n",
       "      <td>18.230000</td>\n",
       "      <td>96.780000</td>\n",
       "      <td>702.880000</td>\n",
       "      <td>0.102730</td>\n",
       "      <td>0.126700</td>\n",
       "      <td>0.193170</td>\n",
       "      <td>0.064690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>29.011492</td>\n",
       "      <td>4.879094</td>\n",
       "      <td>5.192954</td>\n",
       "      <td>23.676089</td>\n",
       "      <td>319.710895</td>\n",
       "      <td>0.014642</td>\n",
       "      <td>0.061144</td>\n",
       "      <td>0.030785</td>\n",
       "      <td>0.008151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.135000</td>\n",
       "      <td>0.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>25.750000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>82.500000</td>\n",
       "      <td>476.750000</td>\n",
       "      <td>0.093500</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.172000</td>\n",
       "      <td>0.059000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>50.500000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>644.000000</td>\n",
       "      <td>0.102000</td>\n",
       "      <td>0.118500</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>0.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>75.250000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>114.250000</td>\n",
       "      <td>917.000000</td>\n",
       "      <td>0.112000</td>\n",
       "      <td>0.157000</td>\n",
       "      <td>0.209000</td>\n",
       "      <td>0.069000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>1878.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id      radius     texture   perimeter         area  \\\n",
       "count  100.000000  100.000000  100.000000  100.000000   100.000000   \n",
       "mean    50.500000   16.850000   18.230000   96.780000   702.880000   \n",
       "std     29.011492    4.879094    5.192954   23.676089   319.710895   \n",
       "min      1.000000    9.000000   11.000000   52.000000   202.000000   \n",
       "25%     25.750000   12.000000   14.000000   82.500000   476.750000   \n",
       "50%     50.500000   17.000000   17.500000   94.000000   644.000000   \n",
       "75%     75.250000   21.000000   22.250000  114.250000   917.000000   \n",
       "max    100.000000   25.000000   27.000000  172.000000  1878.000000   \n",
       "\n",
       "       smoothness  compactness    symmetry  fractal_dimension  \n",
       "count  100.000000   100.000000  100.000000         100.000000  \n",
       "mean     0.102730     0.126700    0.193170           0.064690  \n",
       "std      0.014642     0.061144    0.030785           0.008151  \n",
       "min      0.070000     0.038000    0.135000           0.053000  \n",
       "25%      0.093500     0.080500    0.172000           0.059000  \n",
       "50%      0.102000     0.118500    0.190000           0.063000  \n",
       "75%      0.112000     0.157000    0.209000           0.069000  \n",
       "max      0.143000     0.345000    0.304000           0.097000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'diagnosis_result', 'radius', 'texture', 'perimeter', 'area',\n",
       "       'smoothness', 'compactness', 'symmetry', 'fractal_dimension'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   id                 100 non-null    int64  \n",
      " 1   diagnosis_result   100 non-null    object \n",
      " 2   radius             100 non-null    int64  \n",
      " 3   texture            100 non-null    int64  \n",
      " 4   perimeter          100 non-null    int64  \n",
      " 5   area               100 non-null    int64  \n",
      " 6   smoothness         100 non-null    float64\n",
      " 7   compactness        100 non-null    float64\n",
      " 8   symmetry           100 non-null    float64\n",
      " 9   fractal_dimension  100 non-null    float64\n",
      "dtypes: float64(4), int64(5), object(1)\n",
      "memory usage: 7.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis_result</th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>compactness</th>\n",
       "      <th>symmetry</th>\n",
       "      <th>fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>132</td>\n",
       "      <td>1264</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>B</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>78</td>\n",
       "      <td>451</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>B</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>62</td>\n",
       "      <td>295</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>B</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>413</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100</td>\n",
       "      <td>M</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>94</td>\n",
       "      <td>643</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id diagnosis_result  radius  texture  perimeter  area  smoothness  \\\n",
       "95   96                M      23       16        132  1264       0.091   \n",
       "96   97                B      22       14         78   451       0.105   \n",
       "97   98                B      19       27         62   295       0.102   \n",
       "98   99                B      21       24         74   413       0.090   \n",
       "99  100                M      16       27         94   643       0.098   \n",
       "\n",
       "    compactness  symmetry  fractal_dimension  \n",
       "95        0.131     0.210              0.056  \n",
       "96        0.071     0.190              0.066  \n",
       "97        0.053     0.135              0.069  \n",
       "98        0.075     0.162              0.066  \n",
       "99        0.114     0.188              0.064  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis_result</th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>compactness</th>\n",
       "      <th>symmetry</th>\n",
       "      <th>fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>151</td>\n",
       "      <td>954</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>133</td>\n",
       "      <td>1326</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>130</td>\n",
       "      <td>1203</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>78</td>\n",
       "      <td>386</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>135</td>\n",
       "      <td>1297</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis_result  radius  texture  perimeter  area  smoothness  compactness  \\\n",
       "0                M      23       12        151   954       0.143        0.278   \n",
       "1                B       9       13        133  1326       0.143        0.079   \n",
       "2                M      21       27        130  1203       0.125        0.160   \n",
       "3                M      14       16         78   386       0.070        0.284   \n",
       "4                M       9       19        135  1297       0.141        0.133   \n",
       "\n",
       "   symmetry  fractal_dimension  \n",
       "0     0.242              0.079  \n",
       "1     0.181              0.057  \n",
       "2     0.207              0.060  \n",
       "3     0.260              0.097  \n",
       "4     0.181              0.059  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.diagnosis_result = [1 if each == 'M' else 0 for each in df.diagnosis_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis_result</th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>compactness</th>\n",
       "      <th>symmetry</th>\n",
       "      <th>fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>151</td>\n",
       "      <td>954</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>133</td>\n",
       "      <td>1326</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>130</td>\n",
       "      <td>1203</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>78</td>\n",
       "      <td>386</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>135</td>\n",
       "      <td>1297</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   diagnosis_result  radius  texture  perimeter  area  smoothness  \\\n",
       "0                 1      23       12        151   954       0.143   \n",
       "1                 0       9       13        133  1326       0.143   \n",
       "2                 1      21       27        130  1203       0.125   \n",
       "3                 1      14       16         78   386       0.070   \n",
       "4                 1       9       19        135  1297       0.141   \n",
       "\n",
       "   compactness  symmetry  fractal_dimension  \n",
       "0        0.278     0.242              0.079  \n",
       "1        0.079     0.181              0.057  \n",
       "2        0.160     0.207              0.060  \n",
       "3        0.284     0.260              0.097  \n",
       "4        0.133     0.181              0.059  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    62\n",
       "0    38\n",
       "Name: diagnosis_result, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.diagnosis_result.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.diagnosis_result.values\n",
    "x_ori = df.drop(['diagnosis_result'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius</th>\n",
       "      <th>texture</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>area</th>\n",
       "      <th>smoothness</th>\n",
       "      <th>compactness</th>\n",
       "      <th>symmetry</th>\n",
       "      <th>fractal_dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>151</td>\n",
       "      <td>954</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.242</td>\n",
       "      <td>0.079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>133</td>\n",
       "      <td>1326</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>130</td>\n",
       "      <td>1203</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>78</td>\n",
       "      <td>386</td>\n",
       "      <td>0.070</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.260</td>\n",
       "      <td>0.097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>135</td>\n",
       "      <td>1297</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>132</td>\n",
       "      <td>1264</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>78</td>\n",
       "      <td>451</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.190</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>62</td>\n",
       "      <td>295</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>74</td>\n",
       "      <td>413</td>\n",
       "      <td>0.090</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>94</td>\n",
       "      <td>643</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    radius  texture  perimeter  area  smoothness  compactness  symmetry  \\\n",
       "0       23       12        151   954       0.143        0.278     0.242   \n",
       "1        9       13        133  1326       0.143        0.079     0.181   \n",
       "2       21       27        130  1203       0.125        0.160     0.207   \n",
       "3       14       16         78   386       0.070        0.284     0.260   \n",
       "4        9       19        135  1297       0.141        0.133     0.181   \n",
       "..     ...      ...        ...   ...         ...          ...       ...   \n",
       "95      23       16        132  1264       0.091        0.131     0.210   \n",
       "96      22       14         78   451       0.105        0.071     0.190   \n",
       "97      19       27         62   295       0.102        0.053     0.135   \n",
       "98      21       24         74   413       0.090        0.075     0.162   \n",
       "99      16       27         94   643       0.098        0.114     0.188   \n",
       "\n",
       "    fractal_dimension  \n",
       "0               0.079  \n",
       "1               0.057  \n",
       "2               0.060  \n",
       "3               0.097  \n",
       "4               0.059  \n",
       "..                ...  \n",
       "95              0.056  \n",
       "96              0.066  \n",
       "97              0.069  \n",
       "98              0.066  \n",
       "99              0.064  \n",
       "\n",
       "[100 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ori"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据标准化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.875     , 0.0625    , 0.825     , 0.44868735, 1.        ,\n",
       "        0.78175896, 0.63313609, 0.59090909],\n",
       "       [0.        , 0.125     , 0.675     , 0.67064439, 1.        ,\n",
       "        0.13355049, 0.27218935, 0.09090909],\n",
       "       [0.75      , 1.        , 0.65      , 0.59725537, 0.75342466,\n",
       "        0.39739414, 0.4260355 , 0.15909091],\n",
       "       [0.3125    , 0.3125    , 0.21666667, 0.1097852 , 0.        ,\n",
       "        0.80130293, 0.73964497, 1.        ],\n",
       "       [0.        , 0.5       , 0.69166667, 0.65334129, 0.97260274,\n",
       "        0.30944625, 0.27218935, 0.13636364],\n",
       "       [1.        , 0.875     , 0.25833333, 0.16408115, 0.79452055,\n",
       "        0.42996743, 0.43786982, 0.52272727],\n",
       "       [0.4375    , 0.9375    , 0.56666667, 0.5       , 0.34246575,\n",
       "        0.23127036, 0.26035503, 0.09090909],\n",
       "       [0.375     , 0.4375    , 0.31666667, 0.22434368, 0.67123288,\n",
       "        0.41368078, 0.50295858, 0.5       ],\n",
       "       [0.625     , 0.8125    , 0.3       , 0.18973747, 0.78082192,\n",
       "        0.50488599, 0.59171598, 0.47727273],\n",
       "       [1.        , 0.        , 0.26666667, 0.16348449, 0.67123288,\n",
       "        0.65798046, 0.40236686, 0.65909091],\n",
       "       [0.9375    , 0.625     , 0.425     , 0.35560859, 0.16438356,\n",
       "        0.09446254, 0.10650888, 0.09090909],\n",
       "       [0.5       , 0.25      , 0.43333333, 0.34546539, 0.36986301,\n",
       "        0.29641694, 0.28994083, 0.18181818],\n",
       "       [0.3125    , 0.25      , 0.66666667, 0.54952267, 0.36986301,\n",
       "        0.67752443, 0.62130178, 0.56818182],\n",
       "       [0.1875    , 0.6875    , 0.43333333, 0.34665871, 0.19178082,\n",
       "        0.2019544 , 0.29585799, 0.        ],\n",
       "       [0.1875    , 0.125     , 0.35      , 0.22434368, 0.5890411 ,\n",
       "        0.62214984, 0.4260355 , 0.54545455],\n",
       "       [0.8125    , 0.5       , 0.375     , 0.27267303, 0.60273973,\n",
       "        0.39739414, 0.56213018, 0.40909091],\n",
       "       [0.0625    , 0.3125    , 0.35833333, 0.28818616, 0.39726027,\n",
       "        0.11074919, 0.14201183, 0.13636364],\n",
       "       [0.375     , 0.1875    , 0.46666667, 0.35620525, 0.64383562,\n",
       "        0.53420195, 0.47928994, 0.47727273],\n",
       "       [0.6875    , 0.1875    , 0.65      , 0.63126492, 0.38356164,\n",
       "        0.21172638, 0.13609467, 0.02272727],\n",
       "       [0.5       , 0.        , 0.29166667, 0.21718377, 0.38356164,\n",
       "        0.14006515, 0.31952663, 0.11363636],\n",
       "       [0.4375    , 0.1875    , 0.28333333, 0.18973747, 0.52054795,\n",
       "        0.28990228, 0.36686391, 0.34090909],\n",
       "       [0.5       , 0.8125    , 0.06666667, 0.04295943, 0.43835616,\n",
       "        0.08794788, 0.27810651, 0.36363636],\n",
       "       [0.6875    , 1.        , 0.425     , 0.29952267, 0.50684932,\n",
       "        0.5732899 , 0.69230769, 0.38636364],\n",
       "       [0.625     , 0.0625    , 0.70833333, 0.71718377, 0.32876712,\n",
       "        0.20846906, 0.24852071, 0.        ],\n",
       "       [0.        , 0.125     , 0.48333333, 0.41945107, 0.57534247,\n",
       "        0.35179153, 0.38461538, 0.22727273],\n",
       "       [0.625     , 1.        , 0.53333333, 0.42422434, 0.67123288,\n",
       "        0.61889251, 1.        , 0.47727273],\n",
       "       [0.0625    , 0.8125    , 0.375     , 0.26431981, 0.47945205,\n",
       "        0.48534202, 0.53254438, 0.36363636],\n",
       "       [0.4375    , 0.8125    , 0.58333333, 0.53221957, 0.32876712,\n",
       "        0.2247557 , 0.20710059, 0.09090909],\n",
       "       [0.375     , 0.25      , 0.41666667, 0.31622912, 0.52054795,\n",
       "        0.42996743, 0.34319527, 0.27272727],\n",
       "       [0.125     , 0.3125    , 0.525     , 0.44928401, 0.38356164,\n",
       "        0.25407166, 0.23076923, 0.18181818],\n",
       "       [0.125     , 0.6875    , 0.60833333, 0.52863962, 0.49315068,\n",
       "        0.49185668, 0.49112426, 0.20454545],\n",
       "       [0.875     , 0.9375    , 0.21666667, 0.14260143, 0.56164384,\n",
       "        0.3713355 , 0.56213018, 0.56818182],\n",
       "       [0.6875    , 0.4375    , 0.50833333, 0.41587112, 0.68493151,\n",
       "        0.36482085, 0.53254438, 0.25      ],\n",
       "       [0.125     , 0.625     , 0.63333333, 0.57279236, 0.32876712,\n",
       "        0.43648208, 0.29585799, 0.22727273],\n",
       "       [0.4375    , 0.75      , 0.45833333, 0.36097852, 0.46575342,\n",
       "        0.38436482, 0.38461538, 0.27272727],\n",
       "       [0.0625    , 0.125     , 0.48333333, 0.39856802, 0.35616438,\n",
       "        0.31270358, 0.32544379, 0.09090909],\n",
       "       [0.5625    , 0.0625    , 0.35      , 0.2571599 , 0.38356164,\n",
       "        0.23452769, 0.31952663, 0.18181818],\n",
       "       [0.75      , 0.        , 0.25833333, 0.19212411, 0.2739726 ,\n",
       "        0.        , 0.07100592, 0.13636364],\n",
       "       [0.125     , 0.25      , 0.36666667, 0.29653938, 0.32876712,\n",
       "        0.04234528, 0.13017751, 0.04545455],\n",
       "       [0.0625    , 0.1875    , 0.3       , 0.21300716, 0.43835616,\n",
       "        0.28664495, 0.21893491, 0.25      ],\n",
       "       [0.9375    , 0.3125    , 0.28333333, 0.21539379, 0.16438356,\n",
       "        0.07166124, 0.25443787, 0.06818182],\n",
       "       [0.625     , 1.        , 0.16666667, 0.10083532, 0.7260274 ,\n",
       "        0.27361564, 0.32544379, 0.36363636],\n",
       "       [0.125     , 0.        , 0.63333333, 0.53818616, 0.28767123,\n",
       "        0.58957655, 0.56804734, 0.22727273],\n",
       "       [0.375     , 0.625     , 0.29166667, 0.20465394, 0.46575342,\n",
       "        0.34527687, 0.36686391, 0.34090909],\n",
       "       [0.0625    , 0.25      , 0.275     , 0.19689737, 0.36986301,\n",
       "        0.21824104, 0.23668639, 0.20454545],\n",
       "       [0.5625    , 0.        , 0.6       , 0.52147971, 0.54794521,\n",
       "        0.4267101 , 0.33136095, 0.15909091],\n",
       "       [0.8125    , 0.0625    , 0.        , 0.        , 0.21917808,\n",
       "        0.06840391, 0.24852071, 0.27272727],\n",
       "       [0.6875    , 0.1875    , 0.28333333, 0.19868735, 0.63013699,\n",
       "        0.27687296, 0.46153846, 0.34090909],\n",
       "       [0.6875    , 0.625     , 0.21666667, 0.1473747 , 0.45205479,\n",
       "        0.17263844, 0.19526627, 0.15909091],\n",
       "       [1.        , 0.        , 0.29166667, 0.21420048, 0.24657534,\n",
       "        0.12703583, 0.27218935, 0.09090909],\n",
       "       [0.625     , 0.875     , 0.19166667, 0.13484487, 0.21917808,\n",
       "        0.03908795, 0.0887574 , 0.13636364],\n",
       "       [0.625     , 0.6875    , 0.29166667, 0.22076372, 0.09589041,\n",
       "        0.07491857, 0.        , 0.15909091],\n",
       "       [1.        , 0.25      , 0.2       , 0.14081146, 0.17808219,\n",
       "        0.03257329, 0.30769231, 0.18181818],\n",
       "       [0.3125    , 0.9375    , 0.56666667, 0.49582339, 0.61643836,\n",
       "        0.36156352, 0.43786982, 0.22727273],\n",
       "       [0.5625    , 0.875     , 0.375     , 0.3048926 , 0.28767123,\n",
       "        0.10749186, 0.15976331, 0.09090909],\n",
       "       [0.5625    , 0.125     , 0.175     , 0.12350835, 0.34246575,\n",
       "        0.05537459, 0.33727811, 0.13636364],\n",
       "       [0.0625    , 0.5       , 0.61666667, 0.56682578, 0.47945205,\n",
       "        0.28990228, 0.33727811, 0.15909091],\n",
       "       [0.5       , 0.5625    , 0.36666667, 0.27147971, 0.60273973,\n",
       "        0.32247557, 0.40236686, 0.34090909],\n",
       "       [0.8125    , 0.25      , 0.25833333, 0.19391408, 0.15068493,\n",
       "        0.        , 0.27810651, 0.04545455],\n",
       "       [0.875     , 0.9375    , 0.01666667, 0.01372315, 0.38356164,\n",
       "        0.04885993, 0.19526627, 0.43181818],\n",
       "       [0.375     , 0.4375    , 0.10833333, 0.06563246, 0.5890411 ,\n",
       "        0.14006515, 0.82248521, 0.38636364],\n",
       "       [1.        , 0.25      , 0.025     , 0.01193317, 0.73972603,\n",
       "        0.16938111, 0.28402367, 0.34090909],\n",
       "       [0.1875    , 0.6875    , 0.36666667, 0.26491647, 0.47945205,\n",
       "        0.53094463, 0.35502959, 0.45454545],\n",
       "       [0.9375    , 0.375     , 0.05833333, 0.03520286, 0.09589041,\n",
       "        0.16286645, 0.58579882, 0.38636364],\n",
       "       [0.4375    , 0.5       , 0.25833333, 0.17720764, 0.57534247,\n",
       "        0.28664495, 0.33136095, 0.29545455],\n",
       "       [0.125     , 0.625     , 0.375     , 0.27804296, 0.64383562,\n",
       "        0.35830619, 0.35502959, 0.31818182],\n",
       "       [0.1875    , 0.125     , 0.06666667, 0.03997613, 0.46575342,\n",
       "        0.13029316, 0.21893491, 0.36363636],\n",
       "       [0.5625    , 0.0625    , 0.16666667, 0.11455847, 0.15068493,\n",
       "        0.02931596, 0.10059172, 0.09090909],\n",
       "       [0.4375    , 0.375     , 0.05833333, 0.02923628, 0.50684932,\n",
       "        0.33550489, 0.44970414, 0.61363636],\n",
       "       [0.5       , 0.625     , 0.24166667, 0.17959427, 0.38356164,\n",
       "        0.04560261, 0.14201183, 0.09090909],\n",
       "       [0.75      , 0.4375    , 0.6       , 0.55369928, 0.2739726 ,\n",
       "        0.21172638, 0.13609467, 0.04545455],\n",
       "       [0.        , 0.9375    , 0.05833333, 0.02505967, 0.38356164,\n",
       "        0.37459283, 0.32544379, 0.84090909],\n",
       "       [0.75      , 0.0625    , 0.51666667, 0.43377088, 0.50684932,\n",
       "        0.4723127 , 0.34319527, 0.27272727],\n",
       "       [0.8125    , 0.875     , 0.31666667, 0.22792363, 0.42465753,\n",
       "        0.29315961, 0.18343195, 0.29545455],\n",
       "       [0.5625    , 0.125     , 0.225     , 0.16050119, 0.30136986,\n",
       "        0.09771987, 0.21893491, 0.13636364],\n",
       "       [0.75      , 0.4375    , 0.43333333, 0.36754177, 0.30136986,\n",
       "        0.14983713, 0.26627219, 0.02272727],\n",
       "       [0.0625    , 0.375     , 0.3       , 0.21300716, 0.80821918,\n",
       "        0.21824104, 0.62130178, 0.29545455],\n",
       "       [0.125     , 0.625     , 0.56666667, 0.4797136 , 0.50684932,\n",
       "        0.57654723, 0.47337278, 0.31818182],\n",
       "       [0.4375    , 0.4375    , 0.76666667, 0.62231504, 0.80821918,\n",
       "        1.        , 0.92307692, 0.63636364],\n",
       "       [0.8125    , 0.3125    , 0.25833333, 0.18138425, 0.39726027,\n",
       "        0.18566775, 0.21893491, 0.15909091],\n",
       "       [0.0625    , 0.4375    , 0.18333333, 0.11933174, 0.54794521,\n",
       "        0.18241042, 0.28994083, 0.38636364],\n",
       "       [0.5       , 0.625     , 0.28333333, 0.18973747, 0.52054795,\n",
       "        0.37785016, 0.34911243, 0.36363636],\n",
       "       [0.0625    , 0.25      , 1.        , 1.        , 0.49315068,\n",
       "        0.74592834, 0.28402367, 0.34090909],\n",
       "       [0.6875    , 0.1875    , 0.64166667, 0.5548926 , 0.71232877,\n",
       "        0.45928339, 0.16568047, 0.43181818],\n",
       "       [1.        , 0.625     , 0.20833333, 0.14379475, 0.36986301,\n",
       "        0.11074919, 0.43195266, 0.15909091],\n",
       "       [0.3125    , 0.125     , 0.575     , 0.52088305, 0.39726027,\n",
       "        0.21824104, 0.46153846, 0.15909091],\n",
       "       [0.625     , 0.9375    , 0.35      , 0.26610979, 0.32876712,\n",
       "        0.19869707, 0.43195266, 0.06818182],\n",
       "       [0.625     , 0.        , 0.58333333, 0.52147971, 0.2739726 ,\n",
       "        0.27035831, 0.35502959, 0.06818182],\n",
       "       [0.125     , 0.        , 0.23333333, 0.1575179 , 0.24657534,\n",
       "        0.18241042, 0.34319527, 0.25      ],\n",
       "       [0.1875    , 0.75      , 0.36666667, 0.26849642, 0.5890411 ,\n",
       "        0.31270358, 0.4556213 , 0.22727273],\n",
       "       [0.875     , 1.        , 0.35833333, 0.27505967, 0.2739726 ,\n",
       "        0.15635179, 0.20118343, 0.13636364],\n",
       "       [0.0625    , 0.0625    , 0.4       , 0.31384248, 0.30136986,\n",
       "        0.21498371, 0.21893491, 0.18181818],\n",
       "       [0.3125    , 0.1875    , 0.275     , 0.20883055, 0.05479452,\n",
       "        0.04234528, 0.02366864, 0.        ],\n",
       "       [0.0625    , 0.375     , 0.29166667, 0.21062053, 0.43835616,\n",
       "        0.14332248, 0.17159763, 0.09090909],\n",
       "       [0.8125    , 0.9375    , 0.4       , 0.30071599, 0.46575342,\n",
       "        0.38110749, 0.30177515, 0.22727273],\n",
       "       [0.875     , 0.3125    , 0.66666667, 0.63365155, 0.28767123,\n",
       "        0.3029316 , 0.44378698, 0.06818182],\n",
       "       [0.8125    , 0.1875    , 0.21666667, 0.14856802, 0.47945205,\n",
       "        0.10749186, 0.32544379, 0.29545455],\n",
       "       [0.625     , 1.        , 0.08333333, 0.05548926, 0.43835616,\n",
       "        0.04885993, 0.        , 0.36363636],\n",
       "       [0.75      , 0.8125    , 0.18333333, 0.12589499, 0.2739726 ,\n",
       "        0.12052117, 0.15976331, 0.29545455],\n",
       "       [0.4375    , 1.        , 0.35      , 0.26312649, 0.38356164,\n",
       "        0.247557  , 0.31360947, 0.25      ]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = scaler.fit_transform(x_ori)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train test spilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 8)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names = []\n",
    "method_scores = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### logitic reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_reg = LogisticRegression()\n",
    "logic_reg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_reg_score = logic_reg.score(test_x, test_y)\n",
    "logic_reg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names.append('Logitic Regresssion')\n",
    "method_scores.append(logic_reg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = logic_reg.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3],\n",
       "       [ 1, 15]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logic_reg_matrix = confusion_matrix(test_y, predict_y)\n",
    "logic_reg_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "sns.heatmap(logic_reg_matrix, annot=True, linewidths=0.4, linecolor='red', fmt='.0f', ax =ax)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_clf.fit(train_x, train_y)\n",
    "knn_clf_score = knn_clf.score(test_x, test_y)\n",
    "knn_clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names.append('knn classifier')\n",
    "method_scores.append(knn_clf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = knn_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  4],\n",
       "       [ 0, 16]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf_matrix = confusion_matrix(test_y, predict_y)\n",
    "knn_clf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "sns.heatmap(knn_clf_matrix, annot=True, linewidths=0.4, linecolor='red', fmt='.0f', ax =ax)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf.fit(train_x, train_y)\n",
    "svm_clf_score = svm_clf.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names.append('svm classifier')\n",
    "method_scores.append(svm_clf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = svm_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3],\n",
       "       [ 1, 15]], dtype=int64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf_matrix = confusion_matrix(test_y, predict_y)\n",
    "svm_clf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "sns.heatmap(svm_clf_matrix, annot=True, linewidths=0.4, linecolor='red', fmt='.0f', ax =ax)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native Bays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "native_byes = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_byes.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "native_byes_score = native_byes.score(test_x, test_y)\n",
    "native_byes_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names.append('native_byes')\n",
    "method_scores.append(native_byes_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  2],\n",
       "       [ 4, 12]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y = native_byes.predict(test_x)\n",
    "native_byes_matrix = confusion_matrix(test_y, predict_y)\n",
    "native_byes_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "sns.heatmap(native_byes_matrix, annot=True, linewidths=0.4, linecolor='red', fmt='.0f', ax =ax)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier()\n",
    "dt_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_clf_score = dt_clf.score(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.65"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names.append('decision tree')\n",
    "method_scores.append(dt_clf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3],\n",
       "       [ 4, 12]], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y = dt_clf.predict(test_x)\n",
    "dt_clf_metrix = confusion_matrix(test_y, predict_y)\n",
    "dt_clf_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "sns.heatmap(dt_clf_metrix, annot=True, linewidths=0.4, linecolor='red', fmt='.0f', ax =ax)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=500, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf.fit(train_x, train_y)\n",
    "rf_clf_score = rf_clf.score(test_x, test_y)\n",
    "rf_clf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names.append('random forest')\n",
    "method_scores.append(rf_clf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_y = rf_clf.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  3],\n",
       "       [ 2, 14]], dtype=int64)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf_metrix = confusion_matrix(test_y, predict_y)\n",
    "rf_clf_metrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize = (6,5))\n",
    "sns.heatmap(dt_clf_metrix, annot=True, linewidths=0.4, linecolor='red', fmt='.0f', ax =ax)\n",
    "plt.xlabel('predicted value')\n",
    "plt.ylabel('true value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier():\n",
    "    clf = Sequential()   #  初始化神经网络\n",
    "    clf.add(Dense(units = 50, kernel_initializer = 'uniform', activation = 'relu', input_dim =train_x.shape[1]))\n",
    "    clf.add(Dense(units = 10, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    clf.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    clf.compile(optimizer='adam', loss= 'binary_crossentropy', metrics=['accuracy'])\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "66/66 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.4394\n",
      "Epoch 2/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.6931 - accuracy: 0.4848\n",
      "Epoch 3/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.6932 - accuracy: 0.4848\n",
      "Epoch 4/200\n",
      "66/66 [==============================] - 0s 181us/step - loss: 0.6931 - accuracy: 0.4848\n",
      "Epoch 5/200\n",
      "66/66 [==============================] - 0s 257us/step - loss: 0.6931 - accuracy: 0.4848\n",
      "Epoch 6/200\n",
      "66/66 [==============================] - 0s 317us/step - loss: 0.6931 - accuracy: 0.4848\n",
      "Epoch 7/200\n",
      "66/66 [==============================] - 0s 181us/step - loss: 0.6930 - accuracy: 0.4848\n",
      "Epoch 8/200\n",
      "66/66 [==============================] - 0s 302us/step - loss: 0.6929 - accuracy: 0.4848\n",
      "Epoch 9/200\n",
      "66/66 [==============================] - 0s 332us/step - loss: 0.6929 - accuracy: 0.4848\n",
      "Epoch 10/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.6928 - accuracy: 0.5152\n",
      "Epoch 11/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.6926 - accuracy: 0.6818\n",
      "Epoch 12/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.6924 - accuracy: 0.8636\n",
      "Epoch 13/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.6922 - accuracy: 0.6970\n",
      "Epoch 14/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.6919 - accuracy: 0.6970\n",
      "Epoch 15/200\n",
      "66/66 [==============================] - 0s 181us/step - loss: 0.6917 - accuracy: 0.8485\n",
      "Epoch 16/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.6915 - accuracy: 0.8485\n",
      "Epoch 17/200\n",
      "66/66 [==============================] - 0s 287us/step - loss: 0.6912 - accuracy: 0.7576\n",
      "Epoch 18/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.6908 - accuracy: 0.7576\n",
      "Epoch 19/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.6903 - accuracy: 0.7576\n",
      "Epoch 20/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.6898 - accuracy: 0.7576\n",
      "Epoch 21/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.6893 - accuracy: 0.7576\n",
      "Epoch 22/200\n",
      "66/66 [==============================] - 0s 272us/step - loss: 0.6887 - accuracy: 0.7424\n",
      "Epoch 23/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.6880 - accuracy: 0.7424\n",
      "Epoch 24/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.6869 - accuracy: 0.7576\n",
      "Epoch 25/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.6856 - accuracy: 0.8333\n",
      "Epoch 26/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.6843 - accuracy: 0.8788\n",
      "Epoch 27/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.6829 - accuracy: 0.8485\n",
      "Epoch 28/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.6815 - accuracy: 0.7727\n",
      "Epoch 29/200\n",
      "66/66 [==============================] - 0s 181us/step - loss: 0.6797 - accuracy: 0.7424\n",
      "Epoch 30/200\n",
      "66/66 [==============================] - 0s 272us/step - loss: 0.6777 - accuracy: 0.6970\n",
      "Epoch 31/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.6754 - accuracy: 0.7121\n",
      "Epoch 32/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.6732 - accuracy: 0.7576\n",
      "Epoch 33/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.6707 - accuracy: 0.8030\n",
      "Epoch 34/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.6681 - accuracy: 0.8485\n",
      "Epoch 35/200\n",
      "66/66 [==============================] - 0s 317us/step - loss: 0.6652 - accuracy: 0.8485\n",
      "Epoch 36/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.6619 - accuracy: 0.7727\n",
      "Epoch 37/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.6581 - accuracy: 0.7273\n",
      "Epoch 38/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.6552 - accuracy: 0.6667\n",
      "Epoch 39/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.6519 - accuracy: 0.6515\n",
      "Epoch 40/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.6483 - accuracy: 0.6212\n",
      "Epoch 41/200\n",
      "66/66 [==============================] - 0s 302us/step - loss: 0.6438 - accuracy: 0.6667\n",
      "Epoch 42/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.6391 - accuracy: 0.6970\n",
      "Epoch 43/200\n",
      "66/66 [==============================] - 0s 408us/step - loss: 0.6341 - accuracy: 0.7273\n",
      "Epoch 44/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.6294 - accuracy: 0.7576\n",
      "Epoch 45/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.6247 - accuracy: 0.8182\n",
      "Epoch 46/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.6205 - accuracy: 0.8485\n",
      "Epoch 47/200\n",
      "66/66 [==============================] - 0s 332us/step - loss: 0.6158 - accuracy: 0.8485\n",
      "Epoch 48/200\n",
      "66/66 [==============================] - 0s 287us/step - loss: 0.6115 - accuracy: 0.8485\n",
      "Epoch 49/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.6065 - accuracy: 0.8485\n",
      "Epoch 50/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.5999 - accuracy: 0.8485\n",
      "Epoch 51/200\n",
      "66/66 [==============================] - 0s 302us/step - loss: 0.5933 - accuracy: 0.8485\n",
      "Epoch 52/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.5869 - accuracy: 0.8485\n",
      "Epoch 53/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.5821 - accuracy: 0.8485\n",
      "Epoch 54/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.5758 - accuracy: 0.8333\n",
      "Epoch 55/200\n",
      "66/66 [==============================] - 0s 272us/step - loss: 0.5678 - accuracy: 0.8485\n",
      "Epoch 56/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.5615 - accuracy: 0.8485\n",
      "Epoch 57/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.5545 - accuracy: 0.8485\n",
      "Epoch 58/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.5505 - accuracy: 0.8485\n",
      "Epoch 59/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.5456 - accuracy: 0.8485\n",
      "Epoch 60/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.5405 - accuracy: 0.8485\n",
      "Epoch 61/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.5355 - accuracy: 0.8485\n",
      "Epoch 62/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.5293 - accuracy: 0.8485\n",
      "Epoch 63/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.5247 - accuracy: 0.8485\n",
      "Epoch 64/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.5195 - accuracy: 0.8485\n",
      "Epoch 65/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.5144 - accuracy: 0.8485\n",
      "Epoch 66/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.5094 - accuracy: 0.8636\n",
      "Epoch 67/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.5060 - accuracy: 0.8485\n",
      "Epoch 68/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.5035 - accuracy: 0.8485\n",
      "Epoch 69/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.4996 - accuracy: 0.8333\n",
      "Epoch 70/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.4935 - accuracy: 0.8485\n",
      "Epoch 71/200\n",
      "66/66 [==============================] - 0s 257us/step - loss: 0.4881 - accuracy: 0.8485\n",
      "Epoch 72/200\n",
      "66/66 [==============================] - 0s 393us/step - loss: 0.4834 - accuracy: 0.8485\n",
      "Epoch 73/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.4806 - accuracy: 0.8485\n",
      "Epoch 74/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.4776 - accuracy: 0.8485\n",
      "Epoch 75/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.4731 - accuracy: 0.8485\n",
      "Epoch 76/200\n",
      "66/66 [==============================] - 0s 227us/step - loss: 0.4682 - accuracy: 0.8485\n",
      "Epoch 77/200\n",
      "66/66 [==============================] - 0s 317us/step - loss: 0.4634 - accuracy: 0.8485\n",
      "Epoch 78/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.4584 - accuracy: 0.8485\n",
      "Epoch 79/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.4540 - accuracy: 0.8485\n",
      "Epoch 80/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.4504 - accuracy: 0.8485\n",
      "Epoch 81/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.4477 - accuracy: 0.8485\n",
      "Epoch 82/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.4440 - accuracy: 0.8485\n",
      "Epoch 83/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.4405 - accuracy: 0.8485\n",
      "Epoch 84/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.4374 - accuracy: 0.8485\n",
      "Epoch 85/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.4342 - accuracy: 0.8485\n",
      "Epoch 86/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.4318 - accuracy: 0.8636\n",
      "Epoch 87/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.4288 - accuracy: 0.8636\n",
      "Epoch 88/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.4254 - accuracy: 0.8636\n",
      "Epoch 89/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.4208 - accuracy: 0.8485\n",
      "Epoch 90/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.4182 - accuracy: 0.8485\n",
      "Epoch 91/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.4150 - accuracy: 0.8485\n",
      "Epoch 92/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.4117 - accuracy: 0.8485\n",
      "Epoch 93/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.4083 - accuracy: 0.8485\n",
      "Epoch 94/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.4052 - accuracy: 0.8485\n",
      "Epoch 95/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.4023 - accuracy: 0.8485\n",
      "Epoch 96/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.4002 - accuracy: 0.8636\n",
      "Epoch 97/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3981 - accuracy: 0.8788\n",
      "Epoch 98/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3965 - accuracy: 0.8788\n",
      "Epoch 99/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.3954 - accuracy: 0.8636\n",
      "Epoch 100/200\n",
      "66/66 [==============================] - 0s 91us/step - loss: 0.3942 - accuracy: 0.8636\n",
      "Epoch 101/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.3917 - accuracy: 0.8636\n",
      "Epoch 102/200\n",
      "66/66 [==============================] - 0s 363us/step - loss: 0.3900 - accuracy: 0.8636\n",
      "Epoch 103/200\n",
      "66/66 [==============================] - 0s 181us/step - loss: 0.3906 - accuracy: 0.8485\n",
      "Epoch 104/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3893 - accuracy: 0.8485\n",
      "Epoch 105/200\n",
      "66/66 [==============================] - 0s 91us/step - loss: 0.3831 - accuracy: 0.8636\n",
      "Epoch 106/200\n",
      "66/66 [==============================] - 0s 363us/step - loss: 0.3772 - accuracy: 0.8485\n",
      "Epoch 107/200\n",
      "66/66 [==============================] - 0s 227us/step - loss: 0.3705 - accuracy: 0.8485\n",
      "Epoch 108/200\n",
      "66/66 [==============================] - 0s 181us/step - loss: 0.3751 - accuracy: 0.8485\n",
      "Epoch 109/200\n",
      "66/66 [==============================] - 0s 272us/step - loss: 0.3782 - accuracy: 0.8788\n",
      "Epoch 110/200\n",
      "66/66 [==============================] - 0s 227us/step - loss: 0.3785 - accuracy: 0.8636\n",
      "Epoch 111/200\n",
      "66/66 [==============================] - 0s 272us/step - loss: 0.3727 - accuracy: 0.8788\n",
      "Epoch 112/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3672 - accuracy: 0.8636\n",
      "Epoch 113/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.3634 - accuracy: 0.8333\n",
      "Epoch 114/200\n",
      "66/66 [==============================] - 0s 257us/step - loss: 0.3609 - accuracy: 0.8485\n",
      "Epoch 115/200\n",
      "66/66 [==============================] - 0s 257us/step - loss: 0.3614 - accuracy: 0.8636\n",
      "Epoch 116/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.3640 - accuracy: 0.8636\n",
      "Epoch 117/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3644 - accuracy: 0.8636\n",
      "Epoch 118/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.3629 - accuracy: 0.8636\n",
      "Epoch 119/200\n",
      "66/66 [==============================] - 0s 272us/step - loss: 0.3584 - accuracy: 0.8485\n",
      "Epoch 120/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3548 - accuracy: 0.8636\n",
      "Epoch 121/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3535 - accuracy: 0.8485\n",
      "Epoch 122/200\n",
      "66/66 [==============================] - 0s 242us/step - loss: 0.3521 - accuracy: 0.8485\n",
      "Epoch 123/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3508 - accuracy: 0.8485\n",
      "Epoch 124/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3489 - accuracy: 0.8485\n",
      "Epoch 125/200\n",
      "66/66 [==============================] - 0s 257us/step - loss: 0.3481 - accuracy: 0.8788\n",
      "Epoch 126/200\n",
      "66/66 [==============================] - 0s 287us/step - loss: 0.3471 - accuracy: 0.8485\n",
      "Epoch 127/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3459 - accuracy: 0.8485\n",
      "Epoch 128/200\n",
      "66/66 [==============================] - 0s 257us/step - loss: 0.3448 - accuracy: 0.8485\n",
      "Epoch 129/200\n",
      "66/66 [==============================] - 0s 257us/step - loss: 0.3433 - accuracy: 0.8636\n",
      "Epoch 130/200\n",
      "66/66 [==============================] - 0s 181us/step - loss: 0.3415 - accuracy: 0.8788\n",
      "Epoch 131/200\n",
      "66/66 [==============================] - 0s 332us/step - loss: 0.3409 - accuracy: 0.8788\n",
      "Epoch 132/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3422 - accuracy: 0.8485\n",
      "Epoch 133/200\n",
      "66/66 [==============================] - 0s 302us/step - loss: 0.3413 - accuracy: 0.8485\n",
      "Epoch 134/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3401 - accuracy: 0.8485\n",
      "Epoch 135/200\n",
      "66/66 [==============================] - 0s 181us/step - loss: 0.3380 - accuracy: 0.8788\n",
      "Epoch 136/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3369 - accuracy: 0.8636\n",
      "Epoch 137/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3367 - accuracy: 0.8636\n",
      "Epoch 138/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3358 - accuracy: 0.8636\n",
      "Epoch 139/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3363 - accuracy: 0.8788\n",
      "Epoch 140/200\n",
      "66/66 [==============================] - 0s 272us/step - loss: 0.3354 - accuracy: 0.8636\n",
      "Epoch 141/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3333 - accuracy: 0.8788\n",
      "Epoch 142/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3340 - accuracy: 0.8636\n",
      "Epoch 143/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3332 - accuracy: 0.8636\n",
      "Epoch 144/200\n",
      "66/66 [==============================] - 0s 257us/step - loss: 0.3331 - accuracy: 0.8636\n",
      "Epoch 145/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3339 - accuracy: 0.8636\n",
      "Epoch 146/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3298 - accuracy: 0.8939\n",
      "Epoch 147/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3303 - accuracy: 0.8636\n",
      "Epoch 148/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3317 - accuracy: 0.8636\n",
      "Epoch 149/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3336 - accuracy: 0.8485\n",
      "Epoch 150/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3341 - accuracy: 0.8485\n",
      "Epoch 151/200\n",
      "66/66 [==============================] - 0s 91us/step - loss: 0.3362 - accuracy: 0.8636\n",
      "Epoch 152/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3415 - accuracy: 0.8636\n",
      "Epoch 153/200\n",
      "66/66 [==============================] - 0s 408us/step - loss: 0.3453 - accuracy: 0.8636\n",
      "Epoch 154/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.3436 - accuracy: 0.8636\n",
      "Epoch 155/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3365 - accuracy: 0.8636\n",
      "Epoch 156/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.3318 - accuracy: 0.8636\n",
      "Epoch 157/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3253 - accuracy: 0.8788\n",
      "Epoch 158/200\n",
      "66/66 [==============================] - 0s 287us/step - loss: 0.3223 - accuracy: 0.8788\n",
      "Epoch 159/200\n",
      "66/66 [==============================] - 0s 227us/step - loss: 0.3213 - accuracy: 0.8788\n",
      "Epoch 160/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3208 - accuracy: 0.8939\n",
      "Epoch 161/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3204 - accuracy: 0.8939\n",
      "Epoch 162/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.3187 - accuracy: 0.8788\n",
      "Epoch 163/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3192 - accuracy: 0.8636\n",
      "Epoch 164/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3198 - accuracy: 0.8636\n",
      "Epoch 165/200\n",
      "66/66 [==============================] - 0s 272us/step - loss: 0.3192 - accuracy: 0.8636\n",
      "Epoch 166/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3195 - accuracy: 0.8636\n",
      "Epoch 167/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3328 - accuracy: 0.8636\n",
      "Epoch 168/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3403 - accuracy: 0.8788\n",
      "Epoch 169/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3423 - accuracy: 0.8636\n",
      "Epoch 170/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3375 - accuracy: 0.8788\n",
      "Epoch 171/200\n",
      "66/66 [==============================] - 0s 393us/step - loss: 0.3244 - accuracy: 0.8788\n",
      "Epoch 172/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.3178 - accuracy: 0.8939\n",
      "Epoch 173/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3164 - accuracy: 0.8788\n",
      "Epoch 174/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3185 - accuracy: 0.8788\n",
      "Epoch 175/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3178 - accuracy: 0.8788\n",
      "Epoch 176/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3166 - accuracy: 0.8788\n",
      "Epoch 177/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3157 - accuracy: 0.8788\n",
      "Epoch 178/200\n",
      "66/66 [==============================] - 0s 302us/step - loss: 0.3140 - accuracy: 0.8788\n",
      "Epoch 179/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3131 - accuracy: 0.8788\n",
      "Epoch 180/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3115 - accuracy: 0.8788\n",
      "Epoch 181/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3121 - accuracy: 0.8788\n",
      "Epoch 182/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3110 - accuracy: 0.8788\n",
      "Epoch 183/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3113 - accuracy: 0.8636\n",
      "Epoch 184/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3122 - accuracy: 0.8636\n",
      "Epoch 185/200\n",
      "66/66 [==============================] - 0s 151us/step - loss: 0.3214 - accuracy: 0.8636\n",
      "Epoch 186/200\n",
      "66/66 [==============================] - 0s 287us/step - loss: 0.3291 - accuracy: 0.8939\n",
      "Epoch 187/200\n",
      "66/66 [==============================] - 0s 106us/step - loss: 0.3370 - accuracy: 0.8485\n",
      "Epoch 188/200\n",
      "66/66 [==============================] - 0s 287us/step - loss: 0.3392 - accuracy: 0.8485\n",
      "Epoch 189/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3371 - accuracy: 0.8636\n",
      "Epoch 190/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3312 - accuracy: 0.8788\n",
      "Epoch 191/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3248 - accuracy: 0.8939\n",
      "Epoch 192/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.3196 - accuracy: 0.8636\n",
      "Epoch 193/200\n",
      "66/66 [==============================] - 0s 196us/step - loss: 0.3161 - accuracy: 0.8636\n",
      "Epoch 194/200\n",
      "66/66 [==============================] - 0s 136us/step - loss: 0.3140 - accuracy: 0.8636\n",
      "Epoch 195/200\n",
      "66/66 [==============================] - 0s 166us/step - loss: 0.3122 - accuracy: 0.8636\n",
      "Epoch 196/200\n",
      "66/66 [==============================] - 0s 212us/step - loss: 0.3120 - accuracy: 0.8636\n",
      "Epoch 197/200\n",
      "66/66 [==============================] - 0s 287us/step - loss: 0.3117 - accuracy: 0.8636\n",
      "Epoch 198/200\n",
      "66/66 [==============================] - 0s 227us/step - loss: 0.3106 - accuracy: 0.8636\n",
      "Epoch 199/200\n",
      "66/66 [==============================] - 0s 227us/step - loss: 0.3109 - accuracy: 0.8636\n",
      "Epoch 200/200\n",
      "66/66 [==============================] - 0s 121us/step - loss: 0.3126 - accuracy: 0.8636\n",
      "34/34 [==============================] - 0s 4ms/step\n",
      "Epoch 1/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.4328\n",
      "Epoch 2/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6927 - accuracy: 0.6418\n",
      "Epoch 3/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.6922 - accuracy: 0.6418\n",
      "Epoch 4/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6919 - accuracy: 0.6418\n",
      "Epoch 5/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6915 - accuracy: 0.6418\n",
      "Epoch 6/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6912 - accuracy: 0.6418\n",
      "Epoch 7/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6909 - accuracy: 0.6418\n",
      "Epoch 8/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.6905 - accuracy: 0.6418\n",
      "Epoch 9/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6899 - accuracy: 0.6418\n",
      "Epoch 10/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6894 - accuracy: 0.6418\n",
      "Epoch 11/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6887 - accuracy: 0.6418\n",
      "Epoch 12/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6880 - accuracy: 0.6418\n",
      "Epoch 13/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6870 - accuracy: 0.6418\n",
      "Epoch 14/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6860 - accuracy: 0.6418\n",
      "Epoch 15/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6848 - accuracy: 0.6418\n",
      "Epoch 16/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.6835 - accuracy: 0.6418\n",
      "Epoch 17/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.6820 - accuracy: 0.6418\n",
      "Epoch 18/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.6806 - accuracy: 0.6418\n",
      "Epoch 19/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6792 - accuracy: 0.6418\n",
      "Epoch 20/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6781 - accuracy: 0.6418\n",
      "Epoch 21/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.6764 - accuracy: 0.6418\n",
      "Epoch 22/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.6744 - accuracy: 0.6418\n",
      "Epoch 23/200\n",
      "67/67 [==============================] - 0s 327us/step - loss: 0.6726 - accuracy: 0.6418\n",
      "Epoch 24/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.6700 - accuracy: 0.6418\n",
      "Epoch 25/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.6676 - accuracy: 0.6418\n",
      "Epoch 26/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6646 - accuracy: 0.6418\n",
      "Epoch 27/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.6623 - accuracy: 0.6418\n",
      "Epoch 28/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.6598 - accuracy: 0.6418\n",
      "Epoch 29/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6573 - accuracy: 0.6418\n",
      "Epoch 30/200\n",
      "67/67 [==============================] - 0s 298us/step - loss: 0.6548 - accuracy: 0.6418\n",
      "Epoch 31/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6529 - accuracy: 0.6418\n",
      "Epoch 32/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.6510 - accuracy: 0.6418\n",
      "Epoch 33/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6494 - accuracy: 0.6418\n",
      "Epoch 34/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.6469 - accuracy: 0.6418\n",
      "Epoch 35/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.6445 - accuracy: 0.6418\n",
      "Epoch 36/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6419 - accuracy: 0.6418\n",
      "Epoch 37/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.6389 - accuracy: 0.6418\n",
      "Epoch 38/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.6364 - accuracy: 0.6418\n",
      "Epoch 39/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6332 - accuracy: 0.6418\n",
      "Epoch 40/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6309 - accuracy: 0.6418\n",
      "Epoch 41/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6283 - accuracy: 0.6418\n",
      "Epoch 42/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6260 - accuracy: 0.6418\n",
      "Epoch 43/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6235 - accuracy: 0.6418\n",
      "Epoch 44/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6212 - accuracy: 0.6418\n",
      "Epoch 45/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6193 - accuracy: 0.6418\n",
      "Epoch 46/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6171 - accuracy: 0.6418\n",
      "Epoch 47/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6154 - accuracy: 0.6418\n",
      "Epoch 48/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6133 - accuracy: 0.6418\n",
      "Epoch 49/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6116 - accuracy: 0.6418\n",
      "Epoch 50/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.6103 - accuracy: 0.6418\n",
      "Epoch 51/200\n",
      "67/67 [==============================] - 0s 357us/step - loss: 0.6084 - accuracy: 0.6418\n",
      "Epoch 52/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.6069 - accuracy: 0.6418\n",
      "Epoch 53/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.6056 - accuracy: 0.6418\n",
      "Epoch 54/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.6041 - accuracy: 0.6418\n",
      "Epoch 55/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.6028 - accuracy: 0.6418\n",
      "Epoch 56/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.6013 - accuracy: 0.6418\n",
      "Epoch 57/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.5995 - accuracy: 0.6418\n",
      "Epoch 58/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5978 - accuracy: 0.6418\n",
      "Epoch 59/200\n",
      "67/67 [==============================] - 0s 328us/step - loss: 0.5963 - accuracy: 0.6418\n",
      "Epoch 60/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.5948 - accuracy: 0.6418\n",
      "Epoch 61/200\n",
      "67/67 [==============================] - 0s 89us/step - loss: 0.5937 - accuracy: 0.6418\n",
      "Epoch 62/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.5920 - accuracy: 0.6418\n",
      "Epoch 63/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5904 - accuracy: 0.6418\n",
      "Epoch 64/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.5886 - accuracy: 0.6418\n",
      "Epoch 65/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5871 - accuracy: 0.6418\n",
      "Epoch 66/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5855 - accuracy: 0.6418\n",
      "Epoch 67/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5843 - accuracy: 0.6418\n",
      "Epoch 68/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5827 - accuracy: 0.6418\n",
      "Epoch 69/200\n",
      "67/67 [==============================] - 0s 89us/step - loss: 0.5809 - accuracy: 0.6418\n",
      "Epoch 70/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5793 - accuracy: 0.6418\n",
      "Epoch 71/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5775 - accuracy: 0.6418\n",
      "Epoch 72/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5757 - accuracy: 0.6418\n",
      "Epoch 73/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.5736 - accuracy: 0.6418\n",
      "Epoch 74/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5716 - accuracy: 0.6418\n",
      "Epoch 75/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5694 - accuracy: 0.6418\n",
      "Epoch 76/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.5672 - accuracy: 0.6418\n",
      "Epoch 77/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5652 - accuracy: 0.6418\n",
      "Epoch 78/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5630 - accuracy: 0.6418\n",
      "Epoch 79/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5610 - accuracy: 0.6418\n",
      "Epoch 80/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5591 - accuracy: 0.6418\n",
      "Epoch 81/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5573 - accuracy: 0.6418\n",
      "Epoch 82/200\n",
      "67/67 [==============================] - 0s 2ms/step - loss: 0.5554 - accuracy: 0.6418\n",
      "Epoch 83/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.5535 - accuracy: 0.6418\n",
      "Epoch 84/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5518 - accuracy: 0.6418\n",
      "Epoch 85/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.5500 - accuracy: 0.6418\n",
      "Epoch 86/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5482 - accuracy: 0.6418\n",
      "Epoch 87/200\n",
      "67/67 [==============================] - 0s 298us/step - loss: 0.5459 - accuracy: 0.6418\n",
      "Epoch 88/200\n",
      "67/67 [==============================] - 0s 89us/step - loss: 0.5447 - accuracy: 0.6418\n",
      "Epoch 89/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5423 - accuracy: 0.6418\n",
      "Epoch 90/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.5400 - accuracy: 0.6418\n",
      "Epoch 91/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.5380 - accuracy: 0.6418\n",
      "Epoch 92/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.5370 - accuracy: 0.6418\n",
      "Epoch 93/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.5347 - accuracy: 0.6418\n",
      "Epoch 94/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.5331 - accuracy: 0.6418\n",
      "Epoch 95/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.5312 - accuracy: 0.6418\n",
      "Epoch 96/200\n",
      "67/67 [==============================] - 0s 193us/step - loss: 0.5298 - accuracy: 0.6418\n",
      "Epoch 97/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5277 - accuracy: 0.6418\n",
      "Epoch 98/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5256 - accuracy: 0.6418\n",
      "Epoch 99/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5242 - accuracy: 0.6418\n",
      "Epoch 100/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5232 - accuracy: 0.6418\n",
      "Epoch 101/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5212 - accuracy: 0.6418\n",
      "Epoch 102/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.5195 - accuracy: 0.6418\n",
      "Epoch 103/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.5176 - accuracy: 0.6418\n",
      "Epoch 104/200\n",
      "67/67 [==============================] - 0s 298us/step - loss: 0.5157 - accuracy: 0.6418\n",
      "Epoch 105/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5137 - accuracy: 0.6418\n",
      "Epoch 106/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5119 - accuracy: 0.6418\n",
      "Epoch 107/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5106 - accuracy: 0.6418\n",
      "Epoch 108/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5092 - accuracy: 0.6418\n",
      "Epoch 109/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5080 - accuracy: 0.6418\n",
      "Epoch 110/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5071 - accuracy: 0.6418\n",
      "Epoch 111/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5059 - accuracy: 0.6418\n",
      "Epoch 112/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5048 - accuracy: 0.6418\n",
      "Epoch 113/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5036 - accuracy: 0.6418\n",
      "Epoch 114/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5024 - accuracy: 0.6418\n",
      "Epoch 115/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5014 - accuracy: 0.6418\n",
      "Epoch 116/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5003 - accuracy: 0.6418\n",
      "Epoch 117/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4992 - accuracy: 0.6418\n",
      "Epoch 118/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4978 - accuracy: 0.6418\n",
      "Epoch 119/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4968 - accuracy: 0.6418\n",
      "Epoch 120/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4955 - accuracy: 0.6418\n",
      "Epoch 121/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4943 - accuracy: 0.6418\n",
      "Epoch 122/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.4932 - accuracy: 0.6418\n",
      "Epoch 123/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4927 - accuracy: 0.6418\n",
      "Epoch 124/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4937 - accuracy: 0.7910\n",
      "Epoch 125/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4933 - accuracy: 0.7910\n",
      "Epoch 126/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4919 - accuracy: 0.7910\n",
      "Epoch 127/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.4904 - accuracy: 0.7910\n",
      "Epoch 128/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4884 - accuracy: 0.7910\n",
      "Epoch 129/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4864 - accuracy: 0.7761\n",
      "Epoch 130/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4854 - accuracy: 0.7463\n",
      "Epoch 131/200\n",
      "67/67 [==============================] - 0s 238us/step - loss: 0.4851 - accuracy: 0.7313\n",
      "Epoch 132/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4849 - accuracy: 0.7313\n",
      "Epoch 133/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4847 - accuracy: 0.7313\n",
      "Epoch 134/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4844 - accuracy: 0.7313\n",
      "Epoch 135/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.4833 - accuracy: 0.7313\n",
      "Epoch 136/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.4826 - accuracy: 0.7313\n",
      "Epoch 137/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4811 - accuracy: 0.7313\n",
      "Epoch 138/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.4792 - accuracy: 0.7761\n",
      "Epoch 139/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4782 - accuracy: 0.7761\n",
      "Epoch 140/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4776 - accuracy: 0.7761\n",
      "Epoch 141/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.4768 - accuracy: 0.7761\n",
      "Epoch 142/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4763 - accuracy: 0.7761\n",
      "Epoch 143/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4752 - accuracy: 0.7761\n",
      "Epoch 144/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.4745 - accuracy: 0.7910\n",
      "Epoch 145/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4739 - accuracy: 0.8060\n",
      "Epoch 146/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4734 - accuracy: 0.8060\n",
      "Epoch 147/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4728 - accuracy: 0.8060\n",
      "Epoch 148/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4721 - accuracy: 0.8060\n",
      "Epoch 149/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.4714 - accuracy: 0.8060\n",
      "Epoch 150/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4710 - accuracy: 0.8060\n",
      "Epoch 151/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4709 - accuracy: 0.8060\n",
      "Epoch 152/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4699 - accuracy: 0.8060\n",
      "Epoch 153/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4688 - accuracy: 0.8060\n",
      "Epoch 154/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4677 - accuracy: 0.8060\n",
      "Epoch 155/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4673 - accuracy: 0.8060\n",
      "Epoch 156/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.4666 - accuracy: 0.7910\n",
      "Epoch 157/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4665 - accuracy: 0.7761\n",
      "Epoch 158/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4665 - accuracy: 0.7761\n",
      "Epoch 159/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4658 - accuracy: 0.7761\n",
      "Epoch 160/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4652 - accuracy: 0.7761\n",
      "Epoch 161/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4644 - accuracy: 0.7761\n",
      "Epoch 162/200\n",
      "67/67 [==============================] - 0s 89us/step - loss: 0.4636 - accuracy: 0.7761\n",
      "Epoch 163/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4641 - accuracy: 0.7761\n",
      "Epoch 164/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.4650 - accuracy: 0.7761\n",
      "Epoch 165/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4659 - accuracy: 0.7761\n",
      "Epoch 166/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.4652 - accuracy: 0.7761\n",
      "Epoch 167/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4633 - accuracy: 0.7761\n",
      "Epoch 168/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4614 - accuracy: 0.7761\n",
      "Epoch 169/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.4598 - accuracy: 0.7910\n",
      "Epoch 170/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.4582 - accuracy: 0.7910\n",
      "Epoch 171/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.4568 - accuracy: 0.8060\n",
      "Epoch 172/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4558 - accuracy: 0.8060\n",
      "Epoch 173/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.4552 - accuracy: 0.8060\n",
      "Epoch 174/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4546 - accuracy: 0.8060\n",
      "Epoch 175/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4540 - accuracy: 0.8060\n",
      "Epoch 176/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.4532 - accuracy: 0.8060\n",
      "Epoch 177/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4526 - accuracy: 0.8060\n",
      "Epoch 178/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.4523 - accuracy: 0.8060\n",
      "Epoch 179/200\n",
      "67/67 [==============================] - 0s 238us/step - loss: 0.4517 - accuracy: 0.8060\n",
      "Epoch 180/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4511 - accuracy: 0.8060\n",
      "Epoch 181/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4507 - accuracy: 0.8060\n",
      "Epoch 182/200\n",
      "67/67 [==============================] - 0s 238us/step - loss: 0.4506 - accuracy: 0.8060\n",
      "Epoch 183/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4499 - accuracy: 0.8060\n",
      "Epoch 184/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.4494 - accuracy: 0.8060\n",
      "Epoch 185/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4489 - accuracy: 0.8060\n",
      "Epoch 186/200\n",
      "67/67 [==============================] - 0s 327us/step - loss: 0.4486 - accuracy: 0.8060\n",
      "Epoch 187/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4495 - accuracy: 0.8060\n",
      "Epoch 188/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4498 - accuracy: 0.8060\n",
      "Epoch 189/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.4509 - accuracy: 0.8060\n",
      "Epoch 190/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4492 - accuracy: 0.8060\n",
      "Epoch 191/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.4469 - accuracy: 0.8060\n",
      "Epoch 192/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4439 - accuracy: 0.8060\n",
      "Epoch 193/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.4441 - accuracy: 0.8209\n",
      "Epoch 194/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.4435 - accuracy: 0.8358\n",
      "Epoch 195/200\n",
      "67/67 [==============================] - 0s 298us/step - loss: 0.4440 - accuracy: 0.8657\n",
      "Epoch 196/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4433 - accuracy: 0.8806\n",
      "Epoch 197/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4425 - accuracy: 0.8955\n",
      "Epoch 198/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4439 - accuracy: 0.8955\n",
      "Epoch 199/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4445 - accuracy: 0.8955\n",
      "Epoch 200/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4436 - accuracy: 0.8955\n",
      "33/33 [==============================] - 0s 3ms/step\n",
      "Epoch 1/200\n",
      "67/67 [==============================] - 0s 5ms/step - loss: 0.6931 - accuracy: 0.4627\n",
      "Epoch 2/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6924 - accuracy: 0.7015\n",
      "Epoch 3/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6919 - accuracy: 0.7015\n",
      "Epoch 4/200\n",
      "67/67 [==============================] - 0s 238us/step - loss: 0.6914 - accuracy: 0.7015\n",
      "Epoch 5/200\n",
      "67/67 [==============================] - 0s 238us/step - loss: 0.6910 - accuracy: 0.7015\n",
      "Epoch 6/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6905 - accuracy: 0.7015\n",
      "Epoch 7/200\n",
      "67/67 [==============================] - 0s 298us/step - loss: 0.6900 - accuracy: 0.7015\n",
      "Epoch 8/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.6894 - accuracy: 0.7015\n",
      "Epoch 9/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.6887 - accuracy: 0.7015\n",
      "Epoch 10/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6881 - accuracy: 0.7015\n",
      "Epoch 11/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6874 - accuracy: 0.7015\n",
      "Epoch 12/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.6865 - accuracy: 0.7015\n",
      "Epoch 13/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6855 - accuracy: 0.7015\n",
      "Epoch 14/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6843 - accuracy: 0.7015\n",
      "Epoch 15/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6829 - accuracy: 0.7015\n",
      "Epoch 16/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6814 - accuracy: 0.7015\n",
      "Epoch 17/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6795 - accuracy: 0.7015\n",
      "Epoch 18/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.6775 - accuracy: 0.7015\n",
      "Epoch 19/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6752 - accuracy: 0.7015\n",
      "Epoch 20/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.6727 - accuracy: 0.7015\n",
      "Epoch 21/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6697 - accuracy: 0.7015\n",
      "Epoch 22/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6670 - accuracy: 0.7015\n",
      "Epoch 23/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6636 - accuracy: 0.7015\n",
      "Epoch 24/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.6603 - accuracy: 0.7015\n",
      "Epoch 25/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6563 - accuracy: 0.7015\n",
      "Epoch 26/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.6519 - accuracy: 0.7015\n",
      "Epoch 27/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.6472 - accuracy: 0.7015\n",
      "Epoch 28/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6422 - accuracy: 0.7015\n",
      "Epoch 29/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.6365 - accuracy: 0.7015\n",
      "Epoch 30/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.6315 - accuracy: 0.7015\n",
      "Epoch 31/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.6272 - accuracy: 0.7015\n",
      "Epoch 32/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.6223 - accuracy: 0.7015\n",
      "Epoch 33/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6165 - accuracy: 0.7015\n",
      "Epoch 34/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.6118 - accuracy: 0.7015\n",
      "Epoch 35/200\n",
      "67/67 [==============================] - 0s 89us/step - loss: 0.6058 - accuracy: 0.7015\n",
      "Epoch 36/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.6009 - accuracy: 0.7015\n",
      "Epoch 37/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5962 - accuracy: 0.7015\n",
      "Epoch 38/200\n",
      "67/67 [==============================] - 0s 313us/step - loss: 0.5914 - accuracy: 0.7015\n",
      "Epoch 39/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.5860 - accuracy: 0.7015\n",
      "Epoch 40/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5810 - accuracy: 0.7015\n",
      "Epoch 41/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5767 - accuracy: 0.7015\n",
      "Epoch 42/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5726 - accuracy: 0.7015\n",
      "Epoch 43/200\n",
      "67/67 [==============================] - 0s 89us/step - loss: 0.5688 - accuracy: 0.7015\n",
      "Epoch 44/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5643 - accuracy: 0.7015\n",
      "Epoch 45/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5624 - accuracy: 0.7015\n",
      "Epoch 46/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5604 - accuracy: 0.7015\n",
      "Epoch 47/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5585 - accuracy: 0.7015\n",
      "Epoch 48/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5567 - accuracy: 0.7015\n",
      "Epoch 49/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.5549 - accuracy: 0.7015\n",
      "Epoch 50/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5534 - accuracy: 0.7015\n",
      "Epoch 51/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5515 - accuracy: 0.7015\n",
      "Epoch 52/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5500 - accuracy: 0.7015\n",
      "Epoch 53/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5480 - accuracy: 0.7015\n",
      "Epoch 54/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5461 - accuracy: 0.7015\n",
      "Epoch 55/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5443 - accuracy: 0.7015\n",
      "Epoch 56/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5424 - accuracy: 0.7015\n",
      "Epoch 57/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5403 - accuracy: 0.7015\n",
      "Epoch 58/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5381 - accuracy: 0.7015\n",
      "Epoch 59/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5364 - accuracy: 0.7015\n",
      "Epoch 60/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5344 - accuracy: 0.7015\n",
      "Epoch 61/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5329 - accuracy: 0.7015\n",
      "Epoch 62/200\n",
      "67/67 [==============================] - 0s 417us/step - loss: 0.5313 - accuracy: 0.7015\n",
      "Epoch 63/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.5300 - accuracy: 0.7015\n",
      "Epoch 64/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.5289 - accuracy: 0.7015\n",
      "Epoch 65/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.5270 - accuracy: 0.7015\n",
      "Epoch 66/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5250 - accuracy: 0.7015\n",
      "Epoch 67/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.5228 - accuracy: 0.7015\n",
      "Epoch 68/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.5210 - accuracy: 0.7015\n",
      "Epoch 69/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5193 - accuracy: 0.7015\n",
      "Epoch 70/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5175 - accuracy: 0.7015\n",
      "Epoch 71/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.5156 - accuracy: 0.7015\n",
      "Epoch 72/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.5135 - accuracy: 0.7015\n",
      "Epoch 73/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.5121 - accuracy: 0.7015\n",
      "Epoch 74/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.5097 - accuracy: 0.7015\n",
      "Epoch 75/200\n",
      "67/67 [==============================] - 0s 313us/step - loss: 0.5069 - accuracy: 0.7015\n",
      "Epoch 76/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.5045 - accuracy: 0.7015\n",
      "Epoch 77/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.5018 - accuracy: 0.7015\n",
      "Epoch 78/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4993 - accuracy: 0.7015\n",
      "Epoch 79/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4972 - accuracy: 0.7015\n",
      "Epoch 80/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4942 - accuracy: 0.7015\n",
      "Epoch 81/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4928 - accuracy: 0.7015\n",
      "Epoch 82/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4906 - accuracy: 0.7015\n",
      "Epoch 83/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.4879 - accuracy: 0.7015\n",
      "Epoch 84/200\n",
      "67/67 [==============================] - 0s 327us/step - loss: 0.4846 - accuracy: 0.7015\n",
      "Epoch 85/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.4818 - accuracy: 0.7164\n",
      "Epoch 86/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4787 - accuracy: 0.7164\n",
      "Epoch 87/200\n",
      "67/67 [==============================] - 0s 238us/step - loss: 0.4763 - accuracy: 0.7164\n",
      "Epoch 88/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.4735 - accuracy: 0.7164\n",
      "Epoch 89/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4707 - accuracy: 0.7164\n",
      "Epoch 90/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4676 - accuracy: 0.7164\n",
      "Epoch 91/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.4647 - accuracy: 0.7313\n",
      "Epoch 92/200\n",
      "67/67 [==============================] - 0s 521us/step - loss: 0.4622 - accuracy: 0.7313\n",
      "Epoch 93/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4579 - accuracy: 0.7612\n",
      "Epoch 94/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4549 - accuracy: 0.7761\n",
      "Epoch 95/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4522 - accuracy: 0.7761\n",
      "Epoch 96/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.4498 - accuracy: 0.7761\n",
      "Epoch 97/200\n",
      "67/67 [==============================] - 0s 417us/step - loss: 0.4468 - accuracy: 0.7761\n",
      "Epoch 98/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.4441 - accuracy: 0.7761\n",
      "Epoch 99/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.4411 - accuracy: 0.7910\n",
      "Epoch 100/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.4377 - accuracy: 0.8060\n",
      "Epoch 101/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4339 - accuracy: 0.8209\n",
      "Epoch 102/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4315 - accuracy: 0.8060\n",
      "Epoch 103/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4291 - accuracy: 0.8060\n",
      "Epoch 104/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.4261 - accuracy: 0.8209\n",
      "Epoch 105/200\n",
      "67/67 [==============================] - 0s 387us/step - loss: 0.4234 - accuracy: 0.8209\n",
      "Epoch 106/200\n",
      "67/67 [==============================] - 0s 238us/step - loss: 0.4204 - accuracy: 0.8209\n",
      "Epoch 107/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.4172 - accuracy: 0.8209\n",
      "Epoch 108/200\n",
      "67/67 [==============================] - 0s 253us/step - loss: 0.4149 - accuracy: 0.8358\n",
      "Epoch 109/200\n",
      "67/67 [==============================] - 0s 447us/step - loss: 0.4146 - accuracy: 0.8507\n",
      "Epoch 110/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.4133 - accuracy: 0.8507\n",
      "Epoch 111/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.4107 - accuracy: 0.8507\n",
      "Epoch 112/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.4075 - accuracy: 0.8507\n",
      "Epoch 113/200\n",
      "67/67 [==============================] - 0s 402us/step - loss: 0.4043 - accuracy: 0.8507\n",
      "Epoch 114/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.4017 - accuracy: 0.8507\n",
      "Epoch 115/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.3995 - accuracy: 0.8507\n",
      "Epoch 116/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.3974 - accuracy: 0.8507\n",
      "Epoch 117/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3953 - accuracy: 0.8507\n",
      "Epoch 118/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3930 - accuracy: 0.8507\n",
      "Epoch 119/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.3911 - accuracy: 0.8507\n",
      "Epoch 120/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.3879 - accuracy: 0.8507\n",
      "Epoch 121/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3866 - accuracy: 0.8657\n",
      "Epoch 122/200\n",
      "67/67 [==============================] - 0s 313us/step - loss: 0.3856 - accuracy: 0.8657\n",
      "Epoch 123/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3845 - accuracy: 0.8806\n",
      "Epoch 124/200\n",
      "67/67 [==============================] - 0s 298us/step - loss: 0.3831 - accuracy: 0.8806\n",
      "Epoch 125/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.3815 - accuracy: 0.8955\n",
      "Epoch 126/200\n",
      "67/67 [==============================] - 0s 536us/step - loss: 0.3803 - accuracy: 0.8955\n",
      "Epoch 127/200\n",
      "67/67 [==============================] - 0s 313us/step - loss: 0.3789 - accuracy: 0.9104\n",
      "Epoch 128/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.3780 - accuracy: 0.9104\n",
      "Epoch 129/200\n",
      "67/67 [==============================] - 0s 89us/step - loss: 0.3768 - accuracy: 0.9104\n",
      "Epoch 130/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.3772 - accuracy: 0.9104\n",
      "Epoch 131/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3773 - accuracy: 0.8955\n",
      "Epoch 132/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3777 - accuracy: 0.8955\n",
      "Epoch 133/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3785 - accuracy: 0.8955\n",
      "Epoch 134/200\n",
      "67/67 [==============================] - 0s 387us/step - loss: 0.3771 - accuracy: 0.8955\n",
      "Epoch 135/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.3736 - accuracy: 0.9104\n",
      "Epoch 136/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3704 - accuracy: 0.8955\n",
      "Epoch 137/200\n",
      "67/67 [==============================] - 0s 298us/step - loss: 0.3677 - accuracy: 0.8955\n",
      "Epoch 138/200\n",
      "67/67 [==============================] - ETA: 0s - loss: 0.4409 - accuracy: 0.90 - 0s 417us/step - loss: 0.3656 - accuracy: 0.8955\n",
      "Epoch 139/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3638 - accuracy: 0.8955\n",
      "Epoch 140/200\n",
      "67/67 [==============================] - 0s 372us/step - loss: 0.3608 - accuracy: 0.9104\n",
      "Epoch 141/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3589 - accuracy: 0.9104\n",
      "Epoch 142/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.3565 - accuracy: 0.9104\n",
      "Epoch 143/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3556 - accuracy: 0.8955\n",
      "Epoch 144/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3545 - accuracy: 0.9104\n",
      "Epoch 145/200\n",
      "67/67 [==============================] - 0s 89us/step - loss: 0.3539 - accuracy: 0.9104\n",
      "Epoch 146/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.3536 - accuracy: 0.9104\n",
      "Epoch 147/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.3527 - accuracy: 0.9104\n",
      "Epoch 148/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.3524 - accuracy: 0.9104\n",
      "Epoch 149/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.3513 - accuracy: 0.9104\n",
      "Epoch 150/200\n",
      "67/67 [==============================] - 0s 268us/step - loss: 0.3493 - accuracy: 0.9104\n",
      "Epoch 151/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3479 - accuracy: 0.9104\n",
      "Epoch 152/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3466 - accuracy: 0.9104\n",
      "Epoch 153/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.3457 - accuracy: 0.9104\n",
      "Epoch 154/200\n",
      "67/67 [==============================] - 0s 194us/step - loss: 0.3444 - accuracy: 0.9104\n",
      "Epoch 155/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3432 - accuracy: 0.9104\n",
      "Epoch 156/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3419 - accuracy: 0.9104\n",
      "Epoch 157/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3410 - accuracy: 0.9104\n",
      "Epoch 158/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3396 - accuracy: 0.9104\n",
      "Epoch 159/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3389 - accuracy: 0.9104\n",
      "Epoch 160/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3380 - accuracy: 0.9104\n",
      "Epoch 161/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.3368 - accuracy: 0.9104\n",
      "Epoch 162/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3357 - accuracy: 0.9104\n",
      "Epoch 163/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.3352 - accuracy: 0.8955\n",
      "Epoch 164/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3361 - accuracy: 0.8955\n",
      "Epoch 165/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.3368 - accuracy: 0.8955\n",
      "Epoch 166/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.3386 - accuracy: 0.8806\n",
      "Epoch 167/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.3392 - accuracy: 0.8806\n",
      "Epoch 168/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3393 - accuracy: 0.8806\n",
      "Epoch 169/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3389 - accuracy: 0.8806\n",
      "Epoch 170/200\n",
      "67/67 [==============================] - 0s 372us/step - loss: 0.3378 - accuracy: 0.8806\n",
      "Epoch 171/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3361 - accuracy: 0.8806\n",
      "Epoch 172/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.3332 - accuracy: 0.8955\n",
      "Epoch 173/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.3311 - accuracy: 0.8955\n",
      "Epoch 174/200\n",
      "67/67 [==============================] - 0s 313us/step - loss: 0.3295 - accuracy: 0.8955\n",
      "Epoch 175/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3283 - accuracy: 0.8955\n",
      "Epoch 176/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.3277 - accuracy: 0.9104\n",
      "Epoch 177/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.3270 - accuracy: 0.9104\n",
      "Epoch 178/200\n",
      "67/67 [==============================] - 0s 342us/step - loss: 0.3263 - accuracy: 0.9104\n",
      "Epoch 179/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3259 - accuracy: 0.9104\n",
      "Epoch 180/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3256 - accuracy: 0.9104\n",
      "Epoch 181/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3251 - accuracy: 0.9104\n",
      "Epoch 182/200\n",
      "67/67 [==============================] - 0s 179us/step - loss: 0.3248 - accuracy: 0.9104\n",
      "Epoch 183/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.3253 - accuracy: 0.8955\n",
      "Epoch 184/200\n",
      "67/67 [==============================] - 0s 283us/step - loss: 0.3266 - accuracy: 0.8955\n",
      "Epoch 185/200\n",
      "67/67 [==============================] - 0s 208us/step - loss: 0.3285 - accuracy: 0.8955\n",
      "Epoch 186/200\n",
      "67/67 [==============================] - 0s 164us/step - loss: 0.3303 - accuracy: 0.8806\n",
      "Epoch 187/200\n",
      "67/67 [==============================] - 0s 387us/step - loss: 0.3287 - accuracy: 0.8806\n",
      "Epoch 188/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3253 - accuracy: 0.8955\n",
      "Epoch 189/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3235 - accuracy: 0.8955\n",
      "Epoch 190/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.3224 - accuracy: 0.9104\n",
      "Epoch 191/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.3217 - accuracy: 0.9104\n",
      "Epoch 192/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.3225 - accuracy: 0.8955\n",
      "Epoch 193/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3228 - accuracy: 0.8955\n",
      "Epoch 194/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3240 - accuracy: 0.8955\n",
      "Epoch 195/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3242 - accuracy: 0.8955\n",
      "Epoch 196/200\n",
      "67/67 [==============================] - 0s 149us/step - loss: 0.3239 - accuracy: 0.8955\n",
      "Epoch 197/200\n",
      "67/67 [==============================] - 0s 134us/step - loss: 0.3222 - accuracy: 0.8955\n",
      "Epoch 198/200\n",
      "67/67 [==============================] - 0s 104us/step - loss: 0.3211 - accuracy: 0.8806\n",
      "Epoch 199/200\n",
      "67/67 [==============================] - 0s 119us/step - loss: 0.3194 - accuracy: 0.8955\n",
      "Epoch 200/200\n",
      "67/67 [==============================] - 0s 223us/step - loss: 0.3186 - accuracy: 0.8955\n",
      "33/33 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "clf = KerasClassifier(build_fn = build_classifier, epochs = 200)\n",
    "accuracies = cross_val_score(estimator = clf, X = x, y = y, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8297682801882426"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07601403340994482"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names.append('ANN')\n",
    "method_scores.append(mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Ann 普通模式， 不使用交差验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = build_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.6931 - accuracy: 0.5375\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.6928 - accuracy: 0.5750\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.6926 - accuracy: 0.5750\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.6924 - accuracy: 0.5750\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.6922 - accuracy: 0.5750\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.6919 - accuracy: 0.5750\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.6916 - accuracy: 0.5750\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.6913 - accuracy: 0.5750\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.6910 - accuracy: 0.5750\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.6906 - accuracy: 0.5750\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.6901 - accuracy: 0.5750\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.6897 - accuracy: 0.5750\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.6891 - accuracy: 0.5750\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.6886 - accuracy: 0.5750\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.6878 - accuracy: 0.5750\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.6873 - accuracy: 0.5750\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.6866 - accuracy: 0.5750\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.6856 - accuracy: 0.5750\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.6847 - accuracy: 0.5750\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.6836 - accuracy: 0.5750\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.6825 - accuracy: 0.5750\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.6814 - accuracy: 0.5750\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.6799 - accuracy: 0.5750\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.6787 - accuracy: 0.5750\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.6772 - accuracy: 0.5750\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.6754 - accuracy: 0.5750\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.6738 - accuracy: 0.5750\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.6717 - accuracy: 0.5750\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.6698 - accuracy: 0.5750\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.6674 - accuracy: 0.5750\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.6650 - accuracy: 0.5750\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.6628 - accuracy: 0.5750\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.6607 - accuracy: 0.5750\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.6575 - accuracy: 0.5750\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.6551 - accuracy: 0.5750\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.6521 - accuracy: 0.5750\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 274us/step - loss: 0.6489 - accuracy: 0.5750\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.6459 - accuracy: 0.5750\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.6429 - accuracy: 0.5750\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.6389 - accuracy: 0.5750\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.6351 - accuracy: 0.5750\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.6318 - accuracy: 0.5750\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 174us/step - loss: 0.6278 - accuracy: 0.5875\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.6237 - accuracy: 0.5875\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.6195 - accuracy: 0.5875\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.6150 - accuracy: 0.6000\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.6108 - accuracy: 0.6000\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.6057 - accuracy: 0.6125\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.6010 - accuracy: 0.6375\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.5961 - accuracy: 0.6750\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5907 - accuracy: 0.6750\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.5853 - accuracy: 0.6875\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.5796 - accuracy: 0.7125\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.5738 - accuracy: 0.7250\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.5684 - accuracy: 0.7500\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5625 - accuracy: 0.7500\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.5561 - accuracy: 0.7750\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.5498 - accuracy: 0.7750\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.5436 - accuracy: 0.8000\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.5369 - accuracy: 0.8250\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5300 - accuracy: 0.8375\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5234 - accuracy: 0.8375\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.5166 - accuracy: 0.8375\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.5105 - accuracy: 0.8500\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.5036 - accuracy: 0.8625\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.4971 - accuracy: 0.8500\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4904 - accuracy: 0.8500\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.4839 - accuracy: 0.8500\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4773 - accuracy: 0.8625\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.4714 - accuracy: 0.8625\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.4650 - accuracy: 0.8625\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.4588 - accuracy: 0.8625\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.4528 - accuracy: 0.8625\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4473 - accuracy: 0.8625\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.4412 - accuracy: 0.8625\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4354 - accuracy: 0.8625\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.4309 - accuracy: 0.8625\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.4262 - accuracy: 0.8625\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.4203 - accuracy: 0.8750\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.4153 - accuracy: 0.8750\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.4109 - accuracy: 0.8750\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.4065 - accuracy: 0.8750\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.4027 - accuracy: 0.8750\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 224us/step - loss: 0.3984 - accuracy: 0.8750\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.3941 - accuracy: 0.8750\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.3911 - accuracy: 0.8875\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.3870 - accuracy: 0.8875\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.3837 - accuracy: 0.9000\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.3800 - accuracy: 0.9000\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.3773 - accuracy: 0.9000\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3747 - accuracy: 0.8875\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3712 - accuracy: 0.8875\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3681 - accuracy: 0.9000\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3654 - accuracy: 0.9000\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3632 - accuracy: 0.9000\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3605 - accuracy: 0.9000\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3583 - accuracy: 0.9000\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3557 - accuracy: 0.8875\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.3540 - accuracy: 0.8875\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3516 - accuracy: 0.8875\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 262us/step - loss: 0.3493 - accuracy: 0.8875\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.3479 - accuracy: 0.8875\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.3454 - accuracy: 0.8875\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3434 - accuracy: 0.8875\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3413 - accuracy: 0.8875\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3396 - accuracy: 0.8875\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.3379 - accuracy: 0.9000\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3365 - accuracy: 0.9000\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.3351 - accuracy: 0.8875\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3333 - accuracy: 0.8875\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3314 - accuracy: 0.8875\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3297 - accuracy: 0.8875\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3284 - accuracy: 0.8875\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.3268 - accuracy: 0.8875\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.3253 - accuracy: 0.8875\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.3241 - accuracy: 0.8875\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3227 - accuracy: 0.8875\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.3211 - accuracy: 0.8875\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.3197 - accuracy: 0.8875\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.3186 - accuracy: 0.8875\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.3171 - accuracy: 0.8875\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 287us/step - loss: 0.3161 - accuracy: 0.8875\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.3157 - accuracy: 0.8875\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.3141 - accuracy: 0.8875\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3124 - accuracy: 0.8875\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.3114 - accuracy: 0.9000\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.3102 - accuracy: 0.9000\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.3095 - accuracy: 0.9000\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.3079 - accuracy: 0.9000\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.3067 - accuracy: 0.9000\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.3060 - accuracy: 0.9000\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.3054 - accuracy: 0.9000\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.3042 - accuracy: 0.9000\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.3027 - accuracy: 0.9000\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.3018 - accuracy: 0.9000\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.3007 - accuracy: 0.9000\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2999 - accuracy: 0.9000\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2990 - accuracy: 0.9000\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.2979 - accuracy: 0.9000\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.2970 - accuracy: 0.9000\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.2965 - accuracy: 0.9000\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2952 - accuracy: 0.9000\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2943 - accuracy: 0.9000\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2934 - accuracy: 0.9000\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2927 - accuracy: 0.9000\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.2918 - accuracy: 0.9000\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.2910 - accuracy: 0.9000\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2901 - accuracy: 0.9000\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.2893 - accuracy: 0.9000\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2888 - accuracy: 0.9000\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.2894 - accuracy: 0.9000\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.2876 - accuracy: 0.9000\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2864 - accuracy: 0.9000\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 237us/step - loss: 0.2857 - accuracy: 0.9000\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.2849 - accuracy: 0.9000\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.2856 - accuracy: 0.8875\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 50us/step - loss: 0.2838 - accuracy: 0.8875\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.2837 - accuracy: 0.8875\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2824 - accuracy: 0.9000\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.2819 - accuracy: 0.9000\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.2812 - accuracy: 0.9000\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2813 - accuracy: 0.8875\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2804 - accuracy: 0.8875\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2793 - accuracy: 0.8875\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.2790 - accuracy: 0.8875\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.2784 - accuracy: 0.8875\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.2778 - accuracy: 0.8875\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.2771 - accuracy: 0.8875\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2767 - accuracy: 0.8875\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.2765 - accuracy: 0.8875\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2756 - accuracy: 0.8875\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.2752 - accuracy: 0.8875\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2754 - accuracy: 0.8875\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.2744 - accuracy: 0.9000\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.2739 - accuracy: 0.9000\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.2735 - accuracy: 0.9000\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.2728 - accuracy: 0.9000\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.2724 - accuracy: 0.9000\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 100us/step - loss: 0.2724 - accuracy: 0.9000\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 112us/step - loss: 0.2715 - accuracy: 0.9000\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2724 - accuracy: 0.9000\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.2707 - accuracy: 0.9000\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2704 - accuracy: 0.9000\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 187us/step - loss: 0.2699 - accuracy: 0.9000\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 249us/step - loss: 0.2695 - accuracy: 0.9000\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.2691 - accuracy: 0.9000\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.2688 - accuracy: 0.9000\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2683 - accuracy: 0.9000\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 62us/step - loss: 0.2682 - accuracy: 0.9000\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 162us/step - loss: 0.2676 - accuracy: 0.9000\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 175us/step - loss: 0.2672 - accuracy: 0.9000\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 199us/step - loss: 0.2669 - accuracy: 0.9000\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.2674 - accuracy: 0.8875\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 87us/step - loss: 0.2663 - accuracy: 0.9000\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 137us/step - loss: 0.2659 - accuracy: 0.9000\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 212us/step - loss: 0.2658 - accuracy: 0.9000\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 150us/step - loss: 0.2656 - accuracy: 0.9000\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2657 - accuracy: 0.9000\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - 0s 75us/step - loss: 0.2654 - accuracy: 0.9000\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 125us/step - loss: 0.2648 - accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15aa73c9588>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf1.fit(train_x, train_y, batch_size=40, epochs=200, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = clf1.evaluate(test_x, test_y, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5076435208320618"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], 1))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 8, 1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 8, 1)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, SimpleRNN, Dropout\n",
    "from keras.metrics import mean_squared_error\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "80/80 [==============================] - 2s 24ms/step - loss: 1.4539 - accuracy: 0.3375\n",
      "Epoch 2/200\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.7488 - accuracy: 0.4875\n",
      "Epoch 3/200\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.7297 - accuracy: 0.4125\n",
      "Epoch 4/200\n",
      "80/80 [==============================] - 0s 947us/step - loss: 0.4285 - accuracy: 0.5375\n",
      "Epoch 5/200\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.6040 - accuracy: 0.5000\n",
      "Epoch 6/200\n",
      "80/80 [==============================] - 0s 885us/step - loss: 0.4277 - accuracy: 0.5750\n",
      "Epoch 7/200\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.5790 - accuracy: 0.4125\n",
      "Epoch 8/200\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.4328 - accuracy: 0.5750\n",
      "Epoch 9/200\n",
      "80/80 [==============================] - 0s 885us/step - loss: 0.4330 - accuracy: 0.5000\n",
      "Epoch 10/200\n",
      "80/80 [==============================] - 0s 910us/step - loss: 0.5177 - accuracy: 0.5000\n",
      "Epoch 11/200\n",
      "80/80 [==============================] - 0s 972us/step - loss: 0.5801 - accuracy: 0.4125\n",
      "Epoch 12/200\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.4875\n",
      "Epoch 13/200\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.4849 - accuracy: 0.4750\n",
      "Epoch 14/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4407 - accuracy: 0.4500\n",
      "Epoch 15/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3993 - accuracy: 0.4750\n",
      "Epoch 16/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3525 - accuracy: 0.5500\n",
      "Epoch 17/200\n",
      "80/80 [==============================] - 0s 898us/step - loss: 0.4384 - accuracy: 0.5625\n",
      "Epoch 18/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.3591 - accuracy: 0.5750\n",
      "Epoch 19/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.4970 - accuracy: 0.4625\n",
      "Epoch 20/200\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.4351 - accuracy: 0.5625\n",
      "Epoch 21/200\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.4654 - accuracy: 0.6250\n",
      "Epoch 22/200\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.3108 - accuracy: 0.6125\n",
      "Epoch 23/200\n",
      "80/80 [==============================] - 0s 748us/step - loss: 0.4591 - accuracy: 0.5500\n",
      "Epoch 24/200\n",
      "80/80 [==============================] - 0s 748us/step - loss: 0.4138 - accuracy: 0.4875\n",
      "Epoch 25/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.3816 - accuracy: 0.6375\n",
      "Epoch 26/200\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.3053 - accuracy: 0.6375\n",
      "Epoch 27/200\n",
      "80/80 [==============================] - 0s 835us/step - loss: 0.3206 - accuracy: 0.6500\n",
      "Epoch 28/200\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.3383 - accuracy: 0.6375\n",
      "Epoch 29/200\n",
      "80/80 [==============================] - 0s 898us/step - loss: 0.2706 - accuracy: 0.6750\n",
      "Epoch 30/200\n",
      "80/80 [==============================] - 0s 773us/step - loss: 0.3623 - accuracy: 0.6000\n",
      "Epoch 31/200\n",
      "80/80 [==============================] - 0s 810us/step - loss: 0.2748 - accuracy: 0.7250\n",
      "Epoch 32/200\n",
      "80/80 [==============================] - 0s 985us/step - loss: 0.3189 - accuracy: 0.6375\n",
      "Epoch 33/200\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.2947 - accuracy: 0.6375\n",
      "Epoch 34/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.2858 - accuracy: 0.6125\n",
      "Epoch 35/200\n",
      "80/80 [==============================] - 0s 611us/step - loss: 0.3683 - accuracy: 0.5625\n",
      "Epoch 36/200\n",
      "80/80 [==============================] - 0s 686us/step - loss: 0.3282 - accuracy: 0.6625\n",
      "Epoch 37/200\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.3252 - accuracy: 0.6500\n",
      "Epoch 38/200\n",
      "80/80 [==============================] - 0s 947us/step - loss: 0.3340 - accuracy: 0.5750\n",
      "Epoch 39/200\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.3203 - accuracy: 0.6250\n",
      "Epoch 40/200\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.2863 - accuracy: 0.5875\n",
      "Epoch 41/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.2383 - accuracy: 0.7375\n",
      "Epoch 42/200\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.3802 - accuracy: 0.6375\n",
      "Epoch 43/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.3285 - accuracy: 0.6125\n",
      "Epoch 44/200\n",
      "80/80 [==============================] - 0s 723us/step - loss: 0.2543 - accuracy: 0.6500\n",
      "Epoch 45/200\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.2937 - accuracy: 0.6500\n",
      "Epoch 46/200\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.2438 - accuracy: 0.7000\n",
      "Epoch 47/200\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.3281 - accuracy: 0.5750\n",
      "Epoch 48/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.3166 - accuracy: 0.6500\n",
      "Epoch 49/200\n",
      "80/80 [==============================] - 0s 810us/step - loss: 0.2969 - accuracy: 0.6125\n",
      "Epoch 50/200\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.3192 - accuracy: 0.6625\n",
      "Epoch 51/200\n",
      "80/80 [==============================] - 0s 835us/step - loss: 0.2593 - accuracy: 0.6875\n",
      "Epoch 52/200\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1821 - accuracy: 0.68 - 0s 873us/step - loss: 0.2393 - accuracy: 0.6125\n",
      "Epoch 53/200\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.2675 - accuracy: 0.6750\n",
      "Epoch 54/200\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.2424 - accuracy: 0.6250\n",
      "Epoch 55/200\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.2261 - accuracy: 0.6750\n",
      "Epoch 56/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.2935 - accuracy: 0.6375\n",
      "Epoch 57/200\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.2199 - accuracy: 0.7000\n",
      "Epoch 58/200\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.68 - 0s 798us/step - loss: 0.2942 - accuracy: 0.6500\n",
      "Epoch 59/200\n",
      "80/80 [==============================] - 0s 923us/step - loss: 0.2569 - accuracy: 0.6375\n",
      "Epoch 60/200\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.2687 - accuracy: 0.6500\n",
      "Epoch 61/200\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.2699 - accuracy: 0.6125\n",
      "Epoch 62/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.2433 - accuracy: 0.6875\n",
      "Epoch 63/200\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.2932 - accuracy: 0.6500\n",
      "Epoch 64/200\n",
      "80/80 [==============================] - 0s 910us/step - loss: 0.2123 - accuracy: 0.6750\n",
      "Epoch 65/200\n",
      "80/80 [==============================] - 0s 835us/step - loss: 0.2971 - accuracy: 0.7125\n",
      "Epoch 66/200\n",
      "80/80 [==============================] - 0s 885us/step - loss: 0.1461 - accuracy: 0.8500\n",
      "Epoch 67/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.2029 - accuracy: 0.7500\n",
      "Epoch 68/200\n",
      "80/80 [==============================] - 0s 835us/step - loss: 0.2003 - accuracy: 0.7625\n",
      "Epoch 69/200\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.1677 - accuracy: 0.7250\n",
      "Epoch 70/200\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.2220 - accuracy: 0.6875\n",
      "Epoch 71/200\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.2409 - accuracy: 0.7125\n",
      "Epoch 72/200\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.2604 - accuracy: 0.6750\n",
      "Epoch 73/200\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.2221 - accuracy: 0.7125\n",
      "Epoch 74/200\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.2332 - accuracy: 0.7375\n",
      "Epoch 75/200\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.2005 - accuracy: 0.7750\n",
      "Epoch 76/200\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.2486 - accuracy: 0.6625\n",
      "Epoch 77/200\n",
      "80/80 [==============================] - 0s 898us/step - loss: 0.1812 - accuracy: 0.7375\n",
      "Epoch 78/200\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.1782 - accuracy: 0.8000\n",
      "Epoch 79/200\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.2253 - accuracy: 0.7500\n",
      "Epoch 80/200\n",
      "80/80 [==============================] - 0s 723us/step - loss: 0.2102 - accuracy: 0.7125\n",
      "Epoch 81/200\n",
      "80/80 [==============================] - 0s 748us/step - loss: 0.2148 - accuracy: 0.7250\n",
      "Epoch 82/200\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.1630 - accuracy: 0.8375\n",
      "Epoch 83/200\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.2086 - accuracy: 0.6625\n",
      "Epoch 84/200\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.1885 - accuracy: 0.7375\n",
      "Epoch 85/200\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.2146 - accuracy: 0.6875\n",
      "Epoch 86/200\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.1498 - accuracy: 0.8125\n",
      "Epoch 87/200\n",
      "80/80 [==============================] - 0s 748us/step - loss: 0.1727 - accuracy: 0.7500\n",
      "Epoch 88/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.1557 - accuracy: 0.8125\n",
      "Epoch 89/200\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.1921 - accuracy: 0.7250\n",
      "Epoch 90/200\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.2504 - accuracy: 0.7000\n",
      "Epoch 91/200\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.1941 - accuracy: 0.7000\n",
      "Epoch 92/200\n",
      "80/80 [==============================] - 0s 910us/step - loss: 0.1972 - accuracy: 0.7125\n",
      "Epoch 93/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1776 - accuracy: 0.7750\n",
      "Epoch 94/200\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.1833 - accuracy: 0.7375\n",
      "Epoch 95/200\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.2211 - accuracy: 0.6500\n",
      "Epoch 96/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.1815 - accuracy: 0.8000\n",
      "Epoch 97/200\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.1654 - accuracy: 0.7625\n",
      "Epoch 98/200\n",
      "80/80 [==============================] - 0s 698us/step - loss: 0.1774 - accuracy: 0.8125\n",
      "Epoch 99/200\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.1331 - accuracy: 0.8500\n",
      "Epoch 100/200\n",
      "80/80 [==============================] - 0s 723us/step - loss: 0.1703 - accuracy: 0.7000\n",
      "Epoch 101/200\n",
      "80/80 [==============================] - 0s 923us/step - loss: 0.1697 - accuracy: 0.7625\n",
      "Epoch 102/200\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.1581 - accuracy: 0.8250\n",
      "Epoch 103/200\n",
      "80/80 [==============================] - 0s 885us/step - loss: 0.1856 - accuracy: 0.7250\n",
      "Epoch 104/200\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.1974 - accuracy: 0.7375\n",
      "Epoch 105/200\n",
      "80/80 [==============================] - 0s 910us/step - loss: 0.1450 - accuracy: 0.8250\n",
      "Epoch 106/200\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.2324 - accuracy: 0.7000\n",
      "Epoch 107/200\n",
      "80/80 [==============================] - 0s 611us/step - loss: 0.2180 - accuracy: 0.7125\n",
      "Epoch 108/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.2026 - accuracy: 0.7375\n",
      "Epoch 109/200\n",
      "80/80 [==============================] - 0s 748us/step - loss: 0.1764 - accuracy: 0.7750\n",
      "Epoch 110/200\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.2230 - accuracy: 0.7250\n",
      "Epoch 111/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.1921 - accuracy: 0.7500\n",
      "Epoch 112/200\n",
      "80/80 [==============================] - 0s 985us/step - loss: 0.2479 - accuracy: 0.7000\n",
      "Epoch 113/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.1793 - accuracy: 0.7500\n",
      "Epoch 114/200\n",
      "80/80 [==============================] - 0s 885us/step - loss: 0.2520 - accuracy: 0.6625\n",
      "Epoch 115/200\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.78 - 0s 648us/step - loss: 0.1469 - accuracy: 0.7875\n",
      "Epoch 116/200\n",
      "80/80 [==============================] - 0s 972us/step - loss: 0.2912 - accuracy: 0.5875\n",
      "Epoch 117/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.1406 - accuracy: 0.8000\n",
      "Epoch 118/200\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.1670 - accuracy: 0.7625\n",
      "Epoch 119/200\n",
      "80/80 [==============================] - 0s 885us/step - loss: 0.1333 - accuracy: 0.8375\n",
      "Epoch 120/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.2035 - accuracy: 0.7375\n",
      "Epoch 121/200\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.1631 - accuracy: 0.7875\n",
      "Epoch 122/200\n",
      "80/80 [==============================] - 0s 898us/step - loss: 0.2147 - accuracy: 0.7000\n",
      "Epoch 123/200\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.1203 - accuracy: 0.8750\n",
      "Epoch 124/200\n",
      "80/80 [==============================] - 0s 810us/step - loss: 0.1624 - accuracy: 0.7500\n",
      "Epoch 125/200\n",
      "80/80 [==============================] - 0s 810us/step - loss: 0.1693 - accuracy: 0.8000\n",
      "Epoch 126/200\n",
      "80/80 [==============================] - 0s 785us/step - loss: 0.1300 - accuracy: 0.8125\n",
      "Epoch 127/200\n",
      "80/80 [==============================] - 0s 860us/step - loss: 0.1364 - accuracy: 0.8125\n",
      "Epoch 128/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.1377 - accuracy: 0.8500\n",
      "Epoch 129/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.1661 - accuracy: 0.8500\n",
      "Epoch 130/200\n",
      "80/80 [==============================] - 0s 823us/step - loss: 0.1747 - accuracy: 0.7000\n",
      "Epoch 131/200\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1505 - accuracy: 0.78 - 0s 947us/step - loss: 0.1744 - accuracy: 0.7750\n",
      "Epoch 132/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.1304 - accuracy: 0.8625\n",
      "Epoch 133/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1392 - accuracy: 0.8750\n",
      "Epoch 134/200\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.1667 - accuracy: 0.8000\n",
      "Epoch 135/200\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.1473 - accuracy: 0.7875\n",
      "Epoch 136/200\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.1516 - accuracy: 0.8250\n",
      "Epoch 137/200\n",
      "80/80 [==============================] - 0s 686us/step - loss: 0.1358 - accuracy: 0.8125\n",
      "Epoch 138/200\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.2166 - accuracy: 0.7250\n",
      "Epoch 139/200\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.1501 - accuracy: 0.8000\n",
      "Epoch 140/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.1807 - accuracy: 0.7875\n",
      "Epoch 141/200\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.1775 - accuracy: 0.8000\n",
      "Epoch 142/200\n",
      "80/80 [==============================] - 0s 773us/step - loss: 0.1625 - accuracy: 0.7750\n",
      "Epoch 143/200\n",
      "80/80 [==============================] - 0s 860us/step - loss: 0.1834 - accuracy: 0.8125\n",
      "Epoch 144/200\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1785 - accuracy: 0.78 - 0s 698us/step - loss: 0.1643 - accuracy: 0.8375\n",
      "Epoch 145/200\n",
      "80/80 [==============================] - 0s 736us/step - loss: 0.1612 - accuracy: 0.8250\n",
      "Epoch 146/200\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.1245 - accuracy: 0.8375\n",
      "Epoch 147/200\n",
      "80/80 [==============================] - 0s 760us/step - loss: 0.1821 - accuracy: 0.7375\n",
      "Epoch 148/200\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.1483 - accuracy: 0.7625\n",
      "Epoch 149/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.1821 - accuracy: 0.7750\n",
      "Epoch 150/200\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.1241 - accuracy: 0.8625\n",
      "Epoch 151/200\n",
      "80/80 [==============================] - 0s 611us/step - loss: 0.1916 - accuracy: 0.7375\n",
      "Epoch 152/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.1226 - accuracy: 0.8000\n",
      "Epoch 153/200\n",
      "80/80 [==============================] - 0s 723us/step - loss: 0.1616 - accuracy: 0.8000\n",
      "Epoch 154/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.1494 - accuracy: 0.8125\n",
      "Epoch 155/200\n",
      "80/80 [==============================] - 0s 898us/step - loss: 0.1216 - accuracy: 0.8625\n",
      "Epoch 156/200\n",
      "80/80 [==============================] - 0s 561us/step - loss: 0.1790 - accuracy: 0.7750\n",
      "Epoch 157/200\n",
      "80/80 [==============================] - 0s 860us/step - loss: 0.1557 - accuracy: 0.7625\n",
      "Epoch 158/200\n",
      "80/80 [==============================] - 0s 923us/step - loss: 0.1563 - accuracy: 0.7750\n",
      "Epoch 159/200\n",
      "80/80 [==============================] - 0s 573us/step - loss: 0.1483 - accuracy: 0.8250\n",
      "Epoch 160/200\n",
      "80/80 [==============================] - 0s 810us/step - loss: 0.1347 - accuracy: 0.8500\n",
      "Epoch 161/200\n",
      "80/80 [==============================] - 0s 711us/step - loss: 0.1515 - accuracy: 0.8625\n",
      "Epoch 162/200\n",
      "80/80 [==============================] - 0s 860us/step - loss: 0.1484 - accuracy: 0.8125\n",
      "Epoch 163/200\n",
      "80/80 [==============================] - 0s 686us/step - loss: 0.1472 - accuracy: 0.7875\n",
      "Epoch 164/200\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.1402 - accuracy: 0.8625\n",
      "Epoch 165/200\n",
      "80/80 [==============================] - 0s 623us/step - loss: 0.1536 - accuracy: 0.7750\n",
      "Epoch 166/200\n",
      "80/80 [==============================] - 0s 723us/step - loss: 0.1325 - accuracy: 0.8375\n",
      "Epoch 167/200\n",
      "80/80 [==============================] - 0s 661us/step - loss: 0.1409 - accuracy: 0.8000\n",
      "Epoch 168/200\n",
      "80/80 [==============================] - 0s 598us/step - loss: 0.1076 - accuracy: 0.8375\n",
      "Epoch 169/200\n",
      "80/80 [==============================] - 0s 611us/step - loss: 0.1677 - accuracy: 0.7750\n",
      "Epoch 170/200\n",
      "80/80 [==============================] - 0s 748us/step - loss: 0.1313 - accuracy: 0.7875\n",
      "Epoch 171/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.1440 - accuracy: 0.8500\n",
      "Epoch 172/200\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.1445 - accuracy: 0.78 - 0s 698us/step - loss: 0.1290 - accuracy: 0.8125\n",
      "Epoch 173/200\n",
      "80/80 [==============================] - 0s 947us/step - loss: 0.1634 - accuracy: 0.7875\n",
      "Epoch 174/200\n",
      "80/80 [==============================] - 0s 748us/step - loss: 0.1119 - accuracy: 0.8500\n",
      "Epoch 175/200\n",
      "80/80 [==============================] - 0s 898us/step - loss: 0.1444 - accuracy: 0.7875\n",
      "Epoch 176/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.1417 - accuracy: 0.7750\n",
      "Epoch 177/200\n",
      "80/80 [==============================] - 0s 873us/step - loss: 0.1253 - accuracy: 0.8375\n",
      "Epoch 178/200\n",
      "80/80 [==============================] - 0s 648us/step - loss: 0.1290 - accuracy: 0.8625\n",
      "Epoch 179/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.1211 - accuracy: 0.8625\n",
      "Epoch 180/200\n",
      "80/80 [==============================] - 0s 860us/step - loss: 0.1209 - accuracy: 0.8375\n",
      "Epoch 181/200\n",
      "80/80 [==============================] - 0s 835us/step - loss: 0.1248 - accuracy: 0.8375\n",
      "Epoch 182/200\n",
      "80/80 [==============================] - 0s 923us/step - loss: 0.1357 - accuracy: 0.8250\n",
      "Epoch 183/200\n",
      "80/80 [==============================] - 0s 636us/step - loss: 0.1394 - accuracy: 0.8500\n",
      "Epoch 184/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.1530 - accuracy: 0.7875\n",
      "Epoch 185/200\n",
      "80/80 [==============================] - 0s 898us/step - loss: 0.1030 - accuracy: 0.9000\n",
      "Epoch 186/200\n",
      "80/80 [==============================] - 0s 673us/step - loss: 0.1133 - accuracy: 0.8750\n",
      "Epoch 187/200\n",
      "80/80 [==============================] - 0s 723us/step - loss: 0.1453 - accuracy: 0.8125\n",
      "Epoch 188/200\n",
      "80/80 [==============================] - 0s 835us/step - loss: 0.1554 - accuracy: 0.8000\n",
      "Epoch 189/200\n",
      "80/80 [==============================] - 0s 885us/step - loss: 0.1030 - accuracy: 0.8875\n",
      "Epoch 190/200\n",
      "80/80 [==============================] - 0s 586us/step - loss: 0.1121 - accuracy: 0.8750\n",
      "Epoch 191/200\n",
      "80/80 [==============================] - 0s 835us/step - loss: 0.1252 - accuracy: 0.8750\n",
      "Epoch 192/200\n",
      "80/80 [==============================] - 0s 848us/step - loss: 0.1393 - accuracy: 0.8250\n",
      "Epoch 193/200\n",
      "80/80 [==============================] - 0s 910us/step - loss: 0.1378 - accuracy: 0.8375\n",
      "Epoch 194/200\n",
      "80/80 [==============================] - 0s 935us/step - loss: 0.1067 - accuracy: 0.8875\n",
      "Epoch 195/200\n",
      "80/80 [==============================] - 0s 860us/step - loss: 0.1331 - accuracy: 0.8500\n",
      "Epoch 196/200\n",
      "80/80 [==============================] - 0s 835us/step - loss: 0.1337 - accuracy: 0.8375\n",
      "Epoch 197/200\n",
      "80/80 [==============================] - 0s 1ms/step - loss: 0.1132 - accuracy: 0.8750\n",
      "Epoch 198/200\n",
      "80/80 [==============================] - 0s 798us/step - loss: 0.1230 - accuracy: 0.8000\n",
      "Epoch 199/200\n",
      "80/80 [==============================] - ETA: 0s - loss: 0.2124 - accuracy: 0.81 - 0s 997us/step - loss: 0.1486 - accuracy: 0.8750\n",
      "Epoch 200/200\n",
      "80/80 [==============================] - 0s 686us/step - loss: 0.1176 - accuracy: 0.8375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15aad45dac8>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Sequential()\n",
    "clf.add(SimpleRNN(units = 100, activation = 'tanh', return_sequences = True, input_shape = (train_x.shape[1], 1)))\n",
    "clf.add(Dropout(0.20))\n",
    "clf.add(SimpleRNN(units = 100, activation='tanh',return_sequences=True))\n",
    "clf.add(Dropout(0.20))\n",
    "clf.add(SimpleRNN(units = 70, activation='tanh', return_sequences= True))\n",
    "clf.add(Dropout(0.20))\n",
    "clf.add(SimpleRNN(units = 50))\n",
    "clf.add(Dropout(0.20))\n",
    "# Add final or output layer\n",
    "clf.add(Dense(units=1))\n",
    "clf.compile(optimizer = 'adam', loss = 'mean_squared_error',metrics = ['accuracy'])\n",
    "clf.fit(train_x, train_y, epochs = 200, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 17ms/step\n"
     ]
    }
   ],
   "source": [
    "loss, accuacy = clf.evaluate(test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15358112752437592"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8500000238418579"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_names.append('RNN')\n",
    "method_scores.append(accuacy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 各分类器结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Method Score')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(11,7))\n",
    "plt.ylim([0.60,0.90])   \n",
    "plt.bar(method_names,method_scores,width=0.5)\n",
    "plt.xlabel('Method Name')\n",
    "plt.ylabel('Method Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
